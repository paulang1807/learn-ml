{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#modeling-steps","title":"Modeling Steps","text":"<ul> <li>Data Preparation - Generate or Read data</li> <li>Separate the independent and dependent variables</li> <li>Handle missing data (Needed if there is missing data)</li> <li>Encode Categorical Data (Needed if there is categorical data)</li> <li>Split training and test data</li> <li>Feature Scaling (Needed only for some models)</li> </ul>"},{"location":"#handle-missing-data","title":"Handle missing data","text":"<p>Common Approach - Replace missing value with average of all values in the column</p> <p>SkLearn API</p> <p>Simple Imputer</p> <p>Sample Code <pre><code>from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:, 1:3])  # Include all numeric columns\nX[:, 1:3] = imputer.transform(X[:, 1:3])\n</code></pre></p>"},{"location":"#encode-categorical-data","title":"Encode Categorical Data","text":"<ul> <li>This usually pushes the encoded columns to the front of the array</li> </ul> <p>Encoding the Categorical data when order does not matter</p> <p>SkLearn API</p> <p>Column Transformer </p> <p>One Hot Encoder</p> <p>Sample Code <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\n</code></pre></p> <p>Encoding the Categorical data when order matters (e.g. small,medium,large etc.)</p> <p>SkLearn API</p> <p>Label Encoder</p> <p>Sample Code <pre><code>from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\n</code></pre></p>"},{"location":"#split-training-and-test-data","title":"Split training and test data","text":"<p>SkLearn API</p> <p>Train Test Split</p> <p>Sample Code <pre><code>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n</code></pre></p>"},{"location":"#feature-scaling","title":"Feature Scaling","text":"<ul> <li>Used in some ML models(not all) <ul> <li>Models where there is an implicit relation between the dependent and independent variables (e.g. Support Vector Regression Model)</li> </ul> </li> <li>Done in order to avoid some features dominating the other features</li> <li>Feature scaling is always applied to columns</li> <li>Should not be applied to encoded columns<ul> <li>Will result in loss of interpretation (of the original categories) if applied </li> </ul> </li> </ul> <p>Remember</p> <p>Feature Scaling should always be done after splitting the training and test data. The test data should be clean and not a part of the feature scaling process.</p>"},{"location":"#standardization","title":"Standardization","text":"<ul> <li>This will result in all the features taking values between -3 and 3</li> <li>Works all the time irrespective of the distribution of the features  </li> </ul>"},{"location":"#normalization","title":"Normalization","text":"<ul> <li>This will result in all the features taking values between 0 and 1</li> <li>Recommended when the distribution for most of the features are normalized  </li> </ul> <p>SkLearn API</p> <p>Standard Scaler</p> <p>Sample Code <pre><code>from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n# When some independent variables have been encoded\n# Scale only the non-encoded colummns\n# Since the encoded columns are present in the front of the array, we usually just take everything from the index of the first non encoded numerical column\n# In the below code, 'n' is the number of resulting encoded columns after encoding\nX_train[:, n:] = sc.fit_transform(X_train[:, n:])\n\n# When no independent variables have been encoded\n# Apply feature scaling to all independent variables\n# Also apply to dependent variables, if needed\nX_train = sc.fit_transform(X_train)\n</code></pre></p>"},{"location":"#model-evaluation","title":"Model Evaluation","text":""},{"location":"#regression-models","title":"Regression Models","text":"<p>SkLearn API</p> <p>R2 Score</p> <p>Sample Code <pre><code>from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)\n</code></pre></p>"},{"location":"#classification-models","title":"Classification Models","text":""},{"location":"#confusion-matrix","title":"Confusion Matrix","text":"<p>Displays a matrix of four categories based on the actual and predicted labels</p> <ul> <li>True positive : actual = 1, predicted = 1</li> <li>False positive : actual = 0, predicted = 1</li> <li>False negative : actual = 1, predicted = 0</li> <li>True negative : actual = 0, predicted = 0</li> </ul> <p>Also see Type I and Type II Errors</p> Predicted Negative Predicted Positive Actual Negative TN FP Actual Positive FN TP <p>SkLearn API</p> <p>Confusion Matrix</p> <p>Sample Code <pre><code>from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)\n</code></pre></p>"},{"location":"#accuracy","title":"Accuracy","text":"<p>Fractions of samples predicted correctly</p> <p>SkLearn API</p> <p>Accuracy Score</p> <p>Sample Code <pre><code>from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n</code></pre></p>"},{"location":"#recall","title":"Recall","text":"<p>Fractions of positive events that are predicted correctly</p> <p>SkLearn API</p> <p>Recall Score</p> <p>Sample Code <pre><code>from sklearn.metrics import recall_score\nrecall_score(y_test, y_pred)\n</code></pre></p>"},{"location":"#precision","title":"Precision","text":"<p>Fractions of positive events that are actually positive</p> <p>SkLearn API</p> <p>Precision Score</p> <p>Sample Code <pre><code>from sklearn.metrics import precision_score\nprecision_score(y_test, y_pred)\n</code></pre></p>"},{"location":"#f1-score","title":"F1 Score","text":"<p>Harmonic mean of recall and precision</p> <ul> <li>The higher the score the better the model</li> </ul> <p>SkLearn API</p> <p>F1 Score</p> <p>Sample Code <pre><code>from sklearn.metrics import f1_score\nf1_score(y_test, y_pred)\n</code></pre></p>"},{"location":"#roc-curve-and-roc-auc-score","title":"ROC Curve and ROC AUC Score","text":"<ul> <li>Help with understanding the balance between true positive rate and false positive rates<ul> <li>The area under curve metric helps to analyze the performance</li> </ul> </li> <li>Inputs to these functions are the actual labels and the predicted probabilities (not the predicted labels)</li> <li>ROC stands for Receiver Operating Characteristic</li> <li>The roc curve function returns three lists:<ul> <li>thresholds: all unique prediction probabilities in descending order</li> <li>fpr: the false positive rate (FP / (FP + TN)) for each threshold</li> <li>tpr: the true positive rate (TP / (TP + FN)) for each threshold</li> </ul> </li> </ul> <p>SkLearn API</p> <p>ROC Curve</p> <p>Sample Code <pre><code>from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n</code></pre></p> <p>ROC AUC Score</p> <p>Sample Code <pre><code>from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, y_pred_prob)\n</code></pre></p>"},{"location":"#parameter-tuning","title":"Parameter Tuning","text":"<ul> <li>Used for tuning hyperparameters (parameters which are not learnt by the model)</li> </ul>"},{"location":"ml-cheatsheet/","title":"Cheatsheet","text":""},{"location":"ml-cheatsheet/#k-nearest-neighbors","title":"K-Nearest Neighbors","text":""},{"location":"ml-cheatsheet/#euclidean-distance","title":"Euclidean Distance","text":"\\[ \\sqrt {(x_2^2 - x_1^2) + (y_2^2 - y_1^2)} \\]"},{"location":"ml-cheatsheet/#model-evaluation","title":"Model Evaluation","text":""},{"location":"ml-cheatsheet/#accuracy-and-error-rates","title":"Accuracy and Error Rates","text":"<p>Accuracy Rate  where \\(TN\\) and \\(TP\\) are the total true negatives and true positives respectively</p> <p>Error Rate  where \\(FN\\) and \\(FP\\) are the total false negatives and false positives respectively</p>"},{"location":"ml-cheatsheet/#precision-and-recall","title":"Precision and Recall","text":"<p>Precision  </p> <p>Recall(Sensitivity)  </p>"},{"location":"ml-cheatsheet/#f1-score","title":"F1 Score","text":"\\[ F1 = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}} = \\frac{2 * (Precision * Recall)}{Precision + Recall}\\]"},{"location":"ml-cls/","title":"Classification","text":""},{"location":"ml-cls/#model-types","title":"Model Types","text":""},{"location":"ml-cls/#logistic-regression","title":"Logistic Regression","text":"<ul> <li>Predict a categorical dependent variable from a set of independent variables</li> <li>It is a linear classifier</li> <li>Requires Feature Scaling</li> <li>Uses probabilistic approach <ul> <li>Useful for ranking predictions by their probability</li> <li>Provides info on statistical significance of features</li> </ul> </li> </ul> <p>SkLearn API</p> <p>Logistic Regression</p> <p>Sample Code <pre><code>from sklearn.linear_model import LogisticRegression\nc_lr = LogisticRegression(random_state = 0)\nc_lr.fit(X_train, y_train)\ny_pred = c_lr.predict(sc.transform(X_test))\n</code></pre></p>"},{"location":"ml-cls/#k-nearest-neighbor","title":"K-Nearest Neighbor","text":"<ul> <li>Predict which category a random point falls in</li> <li>Not a linear classifier</li> <li>Requires Feature Scaling</li> </ul> <p>SkLearn API</p> <p>K Neighbors Classifier</p> <p>Sample Code <pre><code>from sklearn.neighbors import KNeighborsClassifier\nc_knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\nc_knn.fit(X_train, y_train)\ny_pred = c_knn.predict(sc.transform(X_test))\n</code></pre></p>"},{"location":"ml-cls/#support-vector-machine-svm","title":"Support Vector Machine (SVM)","text":"<ul> <li>Works with both linear and non linear problems</li> <li>Requires Feature Scaling</li> <li>Not preferred for large number of features</li> <li>Linear SVM <ul> <li>Not preferred for non linear problems</li> </ul> </li> <li>Kernel SVM<ul> <li>High performance on non linear problems</li> <li>Not biased by outliers</li> <li>Not sensitive to overfitting</li> </ul> </li> <li>Good for segmentation use cases</li> </ul> <p>SkLearn API</p> <p>Support Vector Classifier</p> <p>Sample Code <pre><code>from sklearn.svm import SVC\nc_svc_rbf = SVC(kernel = 'rbf', random_state=0)   # rbf kernel\nc_svc_rbf.fit(X_train, y_train)\n# Predict\ny_pred_rbf = c_svc_rbf.predict(sc.transform(X_test))\n</code></pre></p>"},{"location":"ml-cls/#naive-bayes","title":"Naive Bayes","text":"<ul> <li>Works best when there are two independent variables</li> <li>Not a linear classifier</li> <li>Not biased by outliers</li> <li>Requires Feature Scaling</li> <li>Uses probabilistic approach<ul> <li>Useful for ranking predictions by their probability</li> </ul> </li> </ul> <p>SkLearn API</p> <p>Gaussian Naive Bayes Classifier</p> <p>Sample Code <pre><code>from sklearn.naive_bayes import GaussianNB\nc_gnb = GaussianNB()\nc_gnb.fit(X_train, y_train)\n# Predict\ny_pred_gnb = c_gnb.predict(sc.transform(X_test))\n</code></pre></p>"},{"location":"ml-cls/#decision-tree-classification","title":"Decision Tree Classification","text":"<ul> <li>Works with both linear and non linear problems</li> <li>Preferred for better interpretability</li> <li>Feature scaling not needed</li> <li>Not good with small datasets<ul> <li>May result in overfitting</li> </ul> </li> </ul> <p>SkLearn API</p> <p>Decision Tree Classifier</p> <p>Sample Code <pre><code>from sklearn.tree import DecisionTreeClassifier\nc_dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nc_dt.fit(X_train, y_train)\n# Predict\ny_pred_dt = c_dt.predict(sc.transform(X_test))\n</code></pre></p>"},{"location":"ml-cls/#random-forest-classification","title":"Random Forest Classification","text":"<ul> <li>Works with both linear and non linear problems</li> <li>Also see Random Forest Regression</li> </ul> <p>SkLearn API</p> <p>Decision Tree Classifier</p> <p>Sample Code <pre><code>from sklearn.ensemble import RandomForestClassifier\nc_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nc_rf.fit(X_train, y_train)\n# Predict\ny_pred_dt = c_rf.predict(sc.transform(X_test))\n</code></pre></p>"},{"location":"ml-clustr/","title":"Clustering","text":""},{"location":"ml-clustr/#model-types","title":"Model Types","text":""},{"location":"ml-clustr/#k-means-clustering","title":"K-Means Clustering","text":"<ul> <li>The SkLearn Model outputs a numpy array with predicted cluster indices corresponding each data point</li> </ul> <p>SkLearn API</p> <p>K-Means Clustering</p> <p>Sample Code <pre><code>from sklearn.cluster import KMeans\n# Calcluate WCSS and plot\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)   # Get WCSS value and append to list\nplt.plot(range(1, 11), wcss)\nplt.show()\n\n# Select the ELBOW based on the above plot and use that as \n# the number of clusters below\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 0)\n\n# Fit model and predict\n# y_means is a numpy array with predicted cluster indices corresponding each data point\ny_kmeans = kmeans.fit_predict(X)\n\nprint(\"Cluster Predictions: \", y_kmeans)   \nprint(\"Cluster Centers: \", kmeans.cluster_centers_)\n\n# Use predicted cluster as index for the source data to plot scatter \nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'blue', label = 'Cluster 1')\n...\nplt.scatter(X[y_kmeans == n, 0], X[y_kmeans == n, 1], s = 100, c = 'geen', label = 'Cluster n')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'red', label = 'Centroids')\nplt.show()\n</code></pre></p>"},{"location":"ml-reg/","title":"Regression","text":""},{"location":"ml-reg/#model-types","title":"Model Types","text":""},{"location":"ml-reg/#simple-linear-regression","title":"Simple Linear Regression","text":"<ul> <li>Predict dependent variable based on a single independent variable</li> <li>Works well on linear pattern</li> <li>Joining data points result in a single line</li> <li>Does not require Feature Scaling</li> </ul> <p>SkLearn API</p> <p>Linear Regression</p> <p>Sample Code <pre><code>from sklearn.linear_model import LinearRegression\nlr = LinearRegression(fit_intercept=True)\nlr.fit(X_train, y_train)\nintercept = lr.intercept_\ncoefficient = lr.coef_\ny_pred = lr.predict(X_test)\n</code></pre></p>"},{"location":"ml-reg/#multiple-linear-regression","title":"Multiple Linear Regression","text":"<ul> <li>Predict dependent variable based on multiple independent variables</li> <li>Works well on linear pattern</li> <li>Joining data points result in a single line</li> <li>Does not require Feature Scaling</li> </ul> <p>Remember</p> <p>For ML models, the categorical variables need to be transformed to dummy variables (using one hot encoding, for example). However, care should be taken not to include all the resulting dummy variables for multiple regression models, as it will result in multicollinearity. This is also referred to as the Dummy Variable Trap. </p> <p>In order to avoid this, we should always omit one dummy variable corresponding to each categorical variable.</p> <p>In the python code, the scikit learn Multiple Regression class automatically takes care of this, so we don't have to omit the dummy variables explicitly</p> <p>Tip</p> <p>In Multiple Linear Regression, there is no need to apply Feature Scaling. </p> <p>The coefficients for the independent variables will put everything on the same scale.</p> <p>Backward Elimination using Statsmodel</p> <p>Statsmodel</p> <p>Sample Code <pre><code>import statsmodels.api as sm\n\n# Here X is the array after one hot encoding the independent variable\nX = X[:, 1:]   # Avoiding the Dummy Variable Trap\n\n# Statsmodel does not take into account the constant (intercept).\n# So we need to add it in the form b0x0 where x0 is an array of 1s\nX = np.append(arr = np.ones((50, 1)).astype(int), values = X, axis = 1)\n\nregressor_OLS = sm.OLS(endog = y, exog = X_optimal).fit()\nregressor_OLS.summary()\n</code></pre></p>"},{"location":"ml-reg/#polynomial-regression","title":"Polynomial Regression","text":"<ul> <li>Predict dependent variable based on single independent variable</li> <li>Works well on non-linear pattern</li> <li>Joining data points result in a single line</li> <li>Does not require Feature Scaling</li> <li>Need to choose the right polynomial degree for a good bias/variance tradeoff</li> </ul> <p>SkLearn API</p> <p>Polynomial Features</p> <p>Sample Code <pre><code>from sklearn.preprocessing import PolynomialFeatures\nnum_features = 4\npf4 = PolynomialFeatures(degree=num_features, include_bias=False)\nX4 = pf4.fit_transform(X_train)\n\nfrom sklearn.linear_model import LinearRegression\nlr4 = LinearRegression()\nlr4.fit(X4, y)\nprint (lr4.intercept_)\nprint (lr4.coef_)\ny_pred = lr4.predict(pf4.transform(X_test))\n</code></pre></p>"},{"location":"ml-reg/#support-vector-regression","title":"Support Vector Regression","text":"<ul> <li>Predict dependent variable based on single independent variable</li> <li>Works very well on non linear problems; works with linear problems as well</li> <li>Not biased by outliers</li> <li>Joining data points result in a single line</li> <li>Requires Feature Scaling</li> </ul> <p>SkLearn API</p> <p>Support Vector Regressor</p> <p>Sample Code <pre><code># Scale Data\nfrom sklearn.preprocessing import StandardScaler\n# Apply feature scaling to independent variables\n# Also apply to dependent variables, if needed\nsc_x = StandardScaler()\nsc_y = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\ny_train = sc_y.fit_transform(y_train)\n\nfrom sklearn.svm import SVR\nr_svr = SVR(kernel = 'rbf')  # Using radial basis function kernel\nr_svr.fit(X_train,y_train)\ny_pred_sc = r_svr.predict(sc_x.transform(X_test))\n# Inverse Transform to get the predicted value in the original scale\ny_pred = sc_y.inverse_transform(y_pred_sc.reshape(-1,1))\n</code></pre></p>"},{"location":"ml-reg/#decision-tree-regression","title":"Decision Tree Regression","text":"<ul> <li>Predict dependent variable based on one or more independent variables<ul> <li>More applicable to datasets with high number of independent variables</li> </ul> </li> <li>Works on both linear / nonlinear problems</li> <li>Does not require Feature Scaling</li> <li>Poor results if dataset is too small<ul> <li>Overfitting can easily occur</li> </ul> </li> </ul> <p>SkLearn API</p> <p>Decision Tree Regressor</p> <p>Sample Code <pre><code>from sklearn.tree import DecisionTreeRegressor\nr_dt = DecisionTreeRegressor(random_state = 0)\nr_dt.fit(X_train,y_train)\ny_pred = r_dt.predict(X_test)\n</code></pre></p>"},{"location":"ml-reg/#random-forest-regression","title":"Random Forest Regression","text":"<ul> <li>Predict dependent variable based on one or more independent variables</li> <li>Is an ensemble method<ul> <li>Create multiple models (such as decision trees) using different combinations of subset of the independent variables</li> <li>Need to choose the number of trees</li> <li>Final prediction is based on an aggregated value (e.g. avg) of all the predictions</li> </ul> </li> <li>Does not require Feature Scaling</li> <li>Difficult to interpret</li> <li>Overfitting can easily occur</li> </ul> <p>SkLearn API</p> <p>Random Forest Regressor</p> <p>Sample Code <pre><code>from sklearn.ensemble import RandomForestRegressor\nr_rf = RandomForestRegressor(n_estimators = 10, random_state = 0)\nr_rf.fit(X_train,y_train)\ny_pred = r_rf.predict(X_test)\n</code></pre></p>"},{"location":"perm-com/","title":"Permutation and Combination","text":""},{"location":"perm-com/#permutation","title":"Permutation","text":"<ul> <li>Number of ways in which we can arrange a set of things and Order is important</li> </ul> <p>  where \\(n\\) is the number of objects to choose from and \\(r\\) is the number of objects actually selected to be arranged</p>"},{"location":"perm-com/#combination","title":"Combination","text":"<ul> <li>Number of ways in which we can arrange a set of things but Order is NOT important</li> </ul> \\[ C(n,r) = {}^nC_r = {}_nC_r = \\binom nr = \\frac{n!}{r!(n - r)!} \\]"},{"location":"perm-com/#binomial-expansion","title":"Binomial Expansion","text":"\\[(q + p)^n = q^n + \\binom n1 q^{n -1}p + \\binom n2 q^{n - 2}p^2 + ... + p^n = \\sum_{x=0}^n \\binom nx p^xq^{n-x} \\]"},{"location":"prob-cheatsheet/","title":"Cheatsheet","text":""},{"location":"prob-cheatsheet/#probability-basics","title":"Probability Basics","text":"<ul> <li> <p>Probability of an event occuring,  </p> </li> <li> <p>For any event E, </p> <ul> <li> \\[ P(E) \\geq \\emptyset \\] </li> <li> \\[ P(\\Omega) = 1 \\] </li> <li> \\[ P(E_1 \\cup E_2 \\cup ...) = \\sum_{i = 1}^{\\infty}P(E_i) \\] </li> <li> \\[P(E) = 1 - P(E')\\] </li> </ul> </li> <li> <p>Total number of outcomes = \\(x^y\\) where \\(x\\) is number of possible outcomes and \\(y\\) is the number of tries</p> </li> <li> <p>Probability of an event not occuring,  </p> </li> <li> <p>If F is a subset of E, then   </p> </li> </ul>"},{"location":"prob-cheatsheet/#partition-of-state-space-omega","title":"Partition of State Space \\(\\Omega\\)","text":"<ul> <li> <p>If \\(B\\subset \\Omega\\) and \\(A_1,A_2,\\cdots, A_N\\) is a partition of \\(\\Omega\\) then,   </p> </li> <li> <p>If \\(A_i\\cap A_j= \\emptyset\\) then   </p> </li> <li> <p>Combining all these gives  </p> </li> </ul>"},{"location":"prob-cheatsheet/#chain-rule","title":"Chain Rule","text":"\\[ P(X_1, X_2, ..., X_n) = P(X_1 | X_2,...,X_n)P(X_2,...,X_n) \\] \\[ P(A,B,C) = P(A|B,C)P(B,C) \\] \\[ = P(A|B,C)P(B|C)P(C) = P(B|A,C)P(C|A)P(A) \\]"},{"location":"prob-cheatsheet/#odds","title":"Odds","text":"<p>If probability \\(P(E) = p\\), then odds  </p>"},{"location":"prob-cheatsheet/#union-and-intersection","title":"Union and Intersection","text":"<ul> <li>A and B (\\(P(A \\cap B)\\)) is the set of outcomes in both A and B, which implies   and </li> <li>A or B (\\(P(A \\cup B)\\)) is the set of outcomes in A or B, which implies   and   </li> <li>If Events A and B are mutually exclusive,   </li> <li>If A and B are complimentary events,  and </li> <li> <p>Addition Rule</p> \\[ P(A \\ or \\ B) = P(A) + P(B) \u2212 P(A \\ and \\ B) \\] \\[ P(A \\cup B) = P(A) + P(B) \u2212 P(A \\cap B) \\] <ul> <li>For mutually exclusive events  </li> <li>For any three events  </li> </ul> </li> </ul>"},{"location":"prob-cheatsheet/#independent-and-dependent-events","title":"Independent and Dependent Events","text":"<ul> <li> <p>Multiplication Rule</p> <ul> <li>For independent events,  </li> </ul> \\[ P(A \\ and \\ B) = P(A \\cap B) = P(A) P(B) \\] \\[ P(A \\cup B) = P(A) + P(B) \u2212 P(A) P(B) \\] \\[ P(X, Y|Z) = P(X|Z)P(Y|Z) \\] <ul> <li>For more than two independent events</li> </ul> \\[ P(A_1 \\cap A_2 \\cap...\\cap A_n) = P(A_1) \\cdot p(A_2) \\cdot ... \\cdot P(A_n) \\] <ul> <li>For dependent events,</li> </ul> \\[ P(A \\ and \\ B) = P(A \\cap B) = P(A) P(B|A) \\] \\[ P(A \\cup B) = P(A) + P(B) \u2212 P(A) P(B|A) \\] <ul> <li>For any three dependent events  </li> </ul> </li> </ul>"},{"location":"prob-cheatsheet/#law-of-total-probability","title":"Law of Total Probability","text":"\\[ P(Y) = P(Y \\cap X) + P(Y \\cap X') \\] \\[ P(Y) = P(Y|X)P(X) + P(Y|!X)P(!X) \\] <p>If \\(X_1, X_2,...,X_n\\) are mutually exclusive and exhaustive, then  </p> <p>For a continuous parameter \\(\\theta\\) in the range \\([a,b]\\) and discrete random data x,  </p>"},{"location":"prob-cheatsheet/#bayes-theorem","title":"Bayes' Theorem","text":"\\[ P(A|B)  = \\frac{p(A \\cap B)}{P(B)}  = \\frac{p(B|A) P(A)}{P(B)} = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|!A)P(!A)}\\] <ul> <li> <p>For any three events  </p> </li> <li> <p>Bayesian Updating</p> <p>Posterior Probability = \\(\\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Sum of products of Likelihood and Prior (also known as Normalizer)}}\\)  where Likelihood is P(Data|Hypothesis), i.e.P(B|A) and Posterior = P(Hypothesis|Data), i.e. P(A|B)</p> </li> </ul>"},{"location":"prob-cheatsheet/#bayes-factor","title":"Bayes Factor","text":"\\[ \\frac{P(D|H)}{P(D|H')} \\] <p>where D is the Data and H is the hypothesis.</p>"},{"location":"prob-cheatsheet/#probability-distributions-for-random-variables","title":"Probability Distributions for Random Variables","text":""},{"location":"prob-cheatsheet/#binomial-probability","title":"Binomial Probability","text":"<p>  where \\(x\\) is the exact number of times we want success, \\(n\\) is the number of independent trials and \\(\\binom nx\\) is the combination \\({}^{n}C_{x}\\) </p> <p>If \\(X\\) and \\(Y\\) are independent random variables and \\(X \\sim Bin(n,p)\\) and \\(Y \\sim Bin(m,p)\\), then  </p>"},{"location":"prob-cheatsheet/#geometric-probability","title":"Geometric Probability","text":"<p>  where \\(n\\) is the number of attempts required to get a success and \\(p\\) is the probability of success.</p> <p>The probability of success in less than \\(n\\) attempts is given by  </p> <p>The probability of success in at most \\(n\\) attempts is given by  </p> <p>The probability of success in more than \\(n\\) attempts is given by  </p> <p>The probability of success in at least \\(n\\) attempts is given by  </p>"},{"location":"prob-cheatsheet/#poisson-probability","title":"Poisson Probability","text":"\\[ P(x) = \\frac{\\lambda^x \\cdot e^{-\\lambda}}{x!} \\] <p>where \\(x\\) is the number of events observed during any particular time interval of length \\(t\\), \\(\\lambda = \\alpha t\\) and \\(\\alpha\\) is the rate of the event process, the expected number of events occuring in unit time</p> <p>If \\(n \\to \\infty\\) and \\(p \\to 0\\) in such a way that \\(np\\) approaches a value \\(\\lambda &gt; 0\\), i.e. in any binomial experiment in which n is large and p is small, \\(b(x;n,p) \\approx p(x; \\lambda)\\). This can be safely applied when \\(n \\gt 50, np \\lt 5, \\lambda = np\\). </p> <p>So we can express the Poisson Probability of a Binomial Random Variable as   </p>"},{"location":"probability/","title":"Basics","text":""},{"location":"probability/#probability-basics","title":"Probability Basics","text":"<ul> <li>Probability of an event that cannot happen or an impossible event is 0</li> <li>Probability of a event that is certain to happen is 1</li> <li>All other probabilities are between 0 and 1</li> <li>All complementary events are mutually exclusive but the opposite is not true</li> <li>The word or is a key word for addition and the word and is a key word for multiplication</li> </ul> <p>Expression</p> <p>P(at least 1 success) = 1 \u2212 P(all failures)</p> <p>P(at least 1 failure) = 1 \u2212 P(all successes)</p>"},{"location":"probability/#partition-of-state-space-omega","title":"Partition of State Space \\(\\Omega\\)","text":"<p>Is a collection of events \\(A_1,A_2,\\cdots, A_N\\) with 3 properties</p> <ol> <li> <p>\\(A_j \\subset \\Omega\\) for all \\(j \\in \\{1,2,3,\\cdots, N\\}\\)</p> </li> <li> <p>\\(A_i \\cap A_j = \\emptyset\\) whenever \\(i \\neq j\\)</p> </li> <li> <p>\\(A_1 \\cup A_2 \\cup \\cdots \\cup A_N = \\Omega\\)</p> </li> </ol> <p>In other words, each event \\(A_j\\) is made up of outcomes in \\(\\Omega\\), No two \\(A_j\\)'s overlap/share the same outcome, together all \\(A_j\\)'s combine to equal \\(\\Omega\\) </p> <p>Partitions allow us to take complex events which may be difficult to address directly and decompose it into simpler events which are easier to deal with.</p>"},{"location":"probability/#union-and-intersection","title":"Union and Intersection","text":"<p>Intersection: The probability that Events A and B both occur is the probability of the intersection of A and B. </p> <ul> <li>The probability of the intersection of Events A and B is denoted by \\(P(A \\cap B)\\).  </li> </ul> <p>Union: The probability that Events A or B occur is the probability of the union of A and B. </p> <ul> <li>The probability of the union of Events A and B is denoted by \\(P(A \\cup B)\\).</li> </ul> <p>Mutually Exclusive Events: If there is no overlap between A and B, they are called mutually exclusive.</p>"},{"location":"probability/#independent-and-dependent-events","title":"Independent and Dependent Events","text":"<p>Independent events: Events that don\u2019t affect one another, like two separate coin flips</p> <p>Dependent events: Events that affect one another, like pulling two cards from a deck without replacing the first card before pulling the second</p> <ul> <li>The total occurrence for the second event will be less than the first event</li> </ul>"},{"location":"probability/#conditional-probability","title":"Conditional Probability","text":"<p>The probability that Event A occurs, given that Event B has occurred, is referred to as conditional probability. The conditional probability of Event A, given Event B, is denoted by \\(P(A|B)\\).</p>"},{"location":"probability/#bayes-theorem","title":"Bayes' Theorem","text":"<p>Used to calculate the probability of an event, given prior knowledge of related events that occurred earlier. </p> <p>Info</p> <p>In order to be able to use Bayes' theorem, the following should be true:</p> <ul> <li> <p>The sample space is partitioned into a set of mutually exclusive events { \\(A_1\\), \\(A_2\\), . . . , \\(A_n\\) }.</p> </li> <li> <p>Within the sample space, there exists an event B, for which P(B) &gt; 0.</p> </li> </ul>"},{"location":"probability/#bayes-factor","title":"Bayes Factor","text":"<p>It is the ratio of the likelihoods</p> <p>Expression</p> <p>Posterior Odds = Bayes Factor * Prior Odds</p> <ul> <li>If BF  &gt; 1  then  the  posterior  odds  are  greater  than  the  prior  odds.   So  the  data provides evidence for the hypothesis.</li> <li>If BF &lt; 1 then the posterior odds are less than the prior odds.  So the data provides evidence against the hypothesis.</li> </ul>"},{"location":"stat-viz/","title":"Vizuals","text":""},{"location":"stat-viz/#creating-histograms","title":"Creating histograms","text":"<ul> <li>Arrange data points in ascending order</li> <li>Calculate range = Max value - Min value</li> <li>Decide number of classes for histogram (number of groups)</li> <li>Calculate class width = Range / Number of classes (Round up if needed)</li> <li>Determine groups:<ul> <li>Group1: Min Val to (MinVal + Class Width)</li> <li>Group 2: Group1 Upper Val to (Group1 Upper Val+ Class Width)</li> <li>\u2026</li> </ul> </li> </ul>"},{"location":"stat-viz/#whisker-plots","title":"Whisker Plots","text":""},{"location":"stat-viz/#density-curves","title":"Density Curves","text":"<ul> <li>The area under a density curve will always represent 100 % of the data, or 1.0. </li> <li>The curve will never dip below the x-axis. </li> </ul>"},{"location":"stats-basics/","title":"Statistics Basics","text":""},{"location":"stats-basics/#median-and-mode","title":"Median and Mode","text":"<p>Median: The value at the middle of the data set when we line up all the data points in order from least to greatest.</p> <p>Sample median: Obtained by first ordering the n observations from smallest to largest (with any repeated values included so that every sample observation appears in the ordered list). Then,</p> <p>Expression</p> \\[\\tilde x = \\begin{cases} \\text{The single middle value if n is odd} = \\biggl(\\frac{n + 1}{2}\\biggl)^{th} \\text{ordered value} \\\\ \\text{The average of the two middle values if n is even} = {average of } \\biggl(\\frac{n}{2}\\biggl)^{th} and \\biggl(\\frac{n}{2} + 1 \\biggl)^{th} \\text{ordered values} \\end{cases} \\] <p>Mode: The most frequently occuring value in the data set</p>"},{"location":"stats-basics/#range-and-quartiles","title":"Range and Quartiles","text":"<p>Range: The difference between the largest and the smallest value in the dataset.</p> <p>Quartiles: A number that divides the data set into quarters. They are expressed as \\(4^{th}\\).</p> <p>Note</p> <ul> <li>The \\(1^{st}\\) quartile, \\(Q_1\\), is equal to 1/4 or 0.25 quantile or 25th percentile and separates the lowest 25 % of data points from the second 25 % . </li> <li>The \\(2^{nd}\\) quartile, \\(Q_2\\), is the <code>median</code>, and it separates the data set into halves. </li> <li>The \\(3^{rd}\\) quartile, \\(Q_3\\), is equal to 3/4 or 0.75 quantile or 75th percentile and separates the third 25 % of data points from the upper 25 % of data points.</li> </ul> <p>Interquartile Range is the difference between the third and the first quartile</p> <p>Expression</p> \\[IQR = Q_3 - Q_1\\] <p>Deciles are expressed as \\(10^{th}\\). For example, \\(3^{rd}\\) decile is equal to 3/10 or 0.3 quantile</p>"},{"location":"stats-basics/#variance-and-standard-deviation","title":"Variance and Standard Deviation","text":"<p>Variance: How far the data is spread from the mean.  </p> <p>Standard Deviation: Square root of the Variance.</p>"},{"location":"stats-basics/#shifting-and-scaling","title":"Shifting and Scaling","text":"<p>Shifting: Whether we add a constant to each data point or subtract a constant from each data point, the mean, median, and mode will change by the same amount, but the range, standard deviation and IQR will stay the same</p> <p>Expression</p> <p>If \\(y_1 = x_1 + c, y_2 = x_2 + c, ..., y_n = x_n + c, \\ then \\ s_y^2 = s_x^2\\)</p> <p>Scaling: If we multiply or divide each data point in a dataset by a constant value, the mean, median, mode, range ,standard deviation and IQR will all get multiplied or divided by the same value</p> <p>Expression</p> <p>If \\(y_1 = cx_1, y_2 = cx_2, ..., y_n = cx_n, \\ then \\ s_y^2 = c^2s_x^2 \\ and  \\ s_y = |c|s_x\\)</p>"},{"location":"stats-basics/#covariance","title":"Covariance","text":"<p>Covariance measures the amount of linear dependence between two random variables, i.e., it reflects the directional relationship (but not the magnitude) between two random variables.</p> <ul> <li>If a positive change in one variable causes a positive change in the other variable (or a negative change in one causes a negative change in the other), then the variables have a positive relationship and we should expect positive covariance. </li> <li>But if a positive change in one variable causes a negative change in the other variable, then the variables have a negative relationship and we should expect negative covariance. </li> <li>And if the variables have no real discernible relationship, we\u2019ll expect a value for covariance that\u2019s close to 0.</li> </ul> <p>Remember</p> <ul> <li>Covariance calculation can\u2019t distinguish between different units of measure (e.g. using hours vs minutes - using minutes will result in a larger covariance value)</li> <li>To correct for covariance\u2019s lack of sensitivity to the units of measure of each variable, we prefer instead to calculate the correlation coefficient</li> </ul> <p>Tip</p> <ul> <li>If \\(X\\) and \\(Y\\) are independent,   </li> </ul>"},{"location":"stats-basics/#correlation","title":"Correlation","text":"<p>Correlation is the degree of the relationship between two random variables. </p> <ul> <li>Correlation, \\(\\rho\\) is defined as the covariance of the standardizations of \\(X\\) and \\(Y\\)</li> <li>Correlation, \\(\\rho\\) is dimensionless (it's a ratio)</li> </ul> <p></p> <p>Tip</p> <ul> <li>varies between -1 and 1</li> <li> <p>\\(\\rho = 1 \\ or \\ -1\\) iff \\(Y = aX + b\\) for some numbers \\(a\\) and \\(b\\) with \\(a \\neq 0\\)</p> <ul> <li>\\(0 \\leq \\rho \\leq 1\\) indicates positive slope for regression line</li> <li>\\(-1 \\leq \\rho \\leq 0\\) indicates negative slope for regression line</li> </ul> </li> <li> <p>the relationship is described as strong if \\(| \\rho | \\geq .8\\), moderate if \\(.5 \\lt |\\rho | \\lt .8\\), and weak if \\(| \\rho| \\leq .5\\)</p> </li> <li>doesn't change whether we add/multiply constants to/with the x or y variables</li> <li>if \\(X\\) and \\(Y\\) are independent, then \\(\\rho = 0\\), but \\(\\rho = 0\\) does not imply independence</li> </ul> <p>The Pearson Correlation Coefficient is one of several correlation coefficients that can be used to measure correlation. </p> <p>Info</p> <p>It makes sense to calculate the Pearson correlation coefficient if all of the following conditions are true: </p> <ul> <li>Both variables are quantitative.</li> <li>The variables are normally distributed or close to normally distributed. </li> <li>There are no outliers. The presence of outliers may significantly skew the data and the resulting Pearson correlation coefficient will not accurately reflect the correlation of the two variables. </li> <li>The relationship between the variables is linear. Pearson correlation is best for data sets that with a reasonably straight trend line.  </li> </ul>"},{"location":"stats-cheatsheet/","title":"Cheatsheet","text":""},{"location":"stats-cheatsheet/#basics","title":"Basics","text":""},{"location":"stats-cheatsheet/#mean","title":"Mean","text":"PopulationSample \\[\\mu = \\frac{x_1 + x_2 + ... + x_N}{N} = \\frac{\\sum_{i = 1}^N x_i}{N}\\] \\[\\overline x = \\frac{x_1 + x_2 + ... + x_n}{n} = \\frac{\\sum_{i = 1}^n x_i}{n}\\] <p>Also see Random Variables: Expected or Mean Value</p>"},{"location":"stats-cheatsheet/#weighted-mean","title":"Weighted Mean","text":"PopulationSample \\[\\mu = \\frac{\\sum_{i=1}^N w_i x_i}{\\sum_{i=1}^N w_i}\\] \\[\\overline x = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\\]"},{"location":"stats-cheatsheet/#grouped-data-mean","title":"Grouped Data Mean","text":"PopulationSample \\[\\mu = \\frac{\\sum_{i=1}^N f_i M_i}{N}\\] <p>where \\(M_i\\) is the midpoint and \\(f_i\\) is the frequency of each class</p> \\[\\overline x = \\frac{\\sum_{i=1}^n f_i M_i}{n}\\] <p>where \\(M_i\\) is the midpoint and \\(f_i\\) is the frequency of each class</p>"},{"location":"stats-cheatsheet/#variance","title":"Variance","text":"PopulationSample \\[\\sigma^2 = \\frac{\\sum_{i=1}^N (x_i - \\mu)^2}{N} \\] <p>Biased</p> \\[s^2 = \\frac{\\sum_{i=1}^n (x_i - \\overline x)^2}{n} \\] <p>Unbiased</p> \\[s_{n-1}^2 = \\frac{\\sum_{i=1}^n (x_i - \\overline x)^2}{n - 1} \\] <p>Also see Random Variables: Variance</p>"},{"location":"stats-cheatsheet/#grouped-data-variance","title":"Grouped Data Variance","text":"PopulationSample \\[\\sigma^2 = \\frac{\\sum_{i=1}^N f_i(M_i - \\mu)^2}{N}\\] \\[s^2 = \\frac{\\sum_{i=1}^n f_i(M_i - \\overline x)^2}{n-1}\\]"},{"location":"stats-cheatsheet/#standard-deviation","title":"Standard Deviation","text":"PopulationSample \\[\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{\\sum_{i=1}^N (x_i - \\mu)^2}{N}} \\] \\[s = \\sqrt{s_{n-1}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline x)^2}{n - 1}} \\]"},{"location":"stats-cheatsheet/#covariance","title":"Covariance","text":"PopulationSample \\[\\sigma_{xy} = cov(x,y) = \\frac{\\sum(x_i - \\mu_x)(y_i - \\mu_y)}{N}\\] \\[s_{xy} = cov(x,y) = \\frac{\\sum(x_i - \\overline x)(y_i - \\overline y)}{n - 1}\\] <p>Also see Random Variables: Covariance</p>"},{"location":"stats-cheatsheet/#variance-covariance-matrix","title":"Variance Covariance Matrix","text":"\\[vcov(x,y) = \\begin{bmatrix} var(x) &amp; cov(x,y) \\\\ cov(x,y) &amp; var(y) \\end{bmatrix}\\] \\[var(X) = \\begin{bmatrix} var(X_1) &amp; cov(X_1, X_2) &amp; \\dots &amp; cov(X_1, X_n) \\\\ cov(X_2, X_1) &amp; var(X_2) &amp; \\dots &amp; cov(X_2, X_n) \\\\ \\vdots &amp; \\ &amp; \\ddots \\\\ cov(X_n, X_1) &amp; cov(X_n, X_2) &amp; \\dots &amp; var(X_n) \\\\ \\end{bmatrix}\\]"},{"location":"stats-cheatsheet/#correlation","title":"Correlation","text":"\\[ r = \\frac{1}{n - 1}\\sum {\\biggl(\\frac{x_i - \\overline x}{S_x}\\biggl)\\biggl(\\frac{y_i - \\overline y}{S_y}\\biggl)} = \\frac{1}{n - 1}\\sum {(z_x)(z_y)}\\] <p>where \\(S_x\\) and \\(S_y\\) are the standard deviations with respect to x and y and \\(z_x\\) and \\(z_y\\) are the z-scores for x and y.</p>"},{"location":"stats-cheatsheet/#pearson-correlation-coefficient","title":"Pearson Correlation Coefficient","text":"PopulationSample \\[\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}\\] <p>where \\(\\sigma_{xy}\\) is the covariance and \\(\\sigma_x\\) and \\(\\sigma_y\\) are the standard deviations of the variables.</p> \\[r_{xy} = \\frac{s_{xy}}{s_xs_y}\\] <p>Also see Random Variables: Correlation</p>"},{"location":"stats-cheatsheet/#correlation-matrix","title":"Correlation Matrix","text":"\\[r(X) = \\begin{bmatrix} 1 &amp; r(X_1, X_2) &amp; \\dots &amp; r(X_1, X_n) \\\\ r(X_2, X_1) &amp; 1 &amp; \\dots &amp; r(X_2, X_n) \\\\ \\vdots &amp; \\ &amp; \\ddots \\\\ r(X_n, X_1) &amp; r(X_n, X_2) &amp; \\dots &amp; 1 \\\\ \\end{bmatrix}\\]"},{"location":"stats-cheatsheet/#z-score","title":"Z-Score","text":"\\[z = \\frac{\\overline x - \\mu}{\\sigma}\\] <p>where \\(x\\) is the data point, \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation.</p>"},{"location":"stats-cheatsheet/#random-variables","title":"Random Variables","text":""},{"location":"stats-cheatsheet/#discrete-random-variables","title":"Discrete Random Variables","text":"<p>Probability Mass Function (pmf), \\(p(x)\\) for a discrete random variable \\(X\\) is given by   where \\(x\\) is a value of the discrete RV \\(X\\) and belongs to the set of real numbers  and maps to the outcomes w and \\((X = x)\\) is the corresponding event. Also,      and      </p>"},{"location":"stats-cheatsheet/#continuous-random-variables","title":"Continuous Random Variables","text":"<p>For any two numbers a and b, the probability density function (pdf) of a continuous RV \\(X\\) is given by  where   for all \\(x\\) and   is the area under the entire graph of \\(f(x)\\). </p> <p>\\(f(x)dx\\) is the probability that \\(X\\) is in an infinitesimal range around \\(x\\) of width \\(dx\\).</p> <p>Also,  </p> <p>Continuous RV at any single value </p>"},{"location":"stats-cheatsheet/#expected-or-mean-value","title":"Expected or Mean value","text":"Discrete RVContinuous RV <p>  where X is a discrete RV with set of possible values D and pmf p(x)</p> <p>If \\(p(x_i)=p(x_j) \\ \\forall \\ i,j\\), i.e. each outcome has the same probability and hence equal likelihood of happening, then          where 1/k is the probability of k terms with equal likelihood </p> <p>For any linear function \\(h(X) = aX + b\\),  </p> \\[ \\mu_X = E(X) = \\int_{-\\infty}^{\\infty}x.f(x) \\ dx \\] \\[ \\mu_{h(X)} = E[h(X)] = \\int_{-\\infty}^{\\infty}h(x).f(x) \\ dx \\]"},{"location":"stats-cheatsheet/#rules-of-expected-value","title":"Rules of Expected Value","text":"\\[ E(X + Y) = E(X) + E(Y) \\] \\[ E(X - Y) = E(X) - E(Y) \\] \\[ E(aX + bY) = aE(X) + bE(Y) \\] <p>If \\(X\\) and \\(Y\\) are independent,  </p> <p>For any linear function \\(h(X) = aX + b\\),  </p>"},{"location":"stats-cheatsheet/#variance_1","title":"Variance","text":"Discrete RVContinuous RV \\[ V(X) = \\sigma_X^2 = \\sum_D(x - \\mu)^2 . p(x) = \\sum_{i=1}^n (x_i - \\mu)^2 . p(x_i) \\] \\[ = E[(x- \\mu)^2] \\] \\[ = \\biggl[\\sum_D x^2 .p(x)\\biggl] - \\mu^2 \\] \\[ = E(X^2) - [E(X)]^2 \\] \\[ = E(X^2) - \\mu^2 \\] <p>For any linear function \\(h(X) = aX + b\\),  </p> \\[ V(X) = \\sigma_X^2 = \\int_{-\\infty}^{\\infty}(x - \\mu)^2 . f(x) \\ dx \\] \\[ = E[(x- \\mu)^2] \\]"},{"location":"stats-cheatsheet/#standard-deviation_1","title":"Standard Deviation","text":"\\[ \\sigma_X = \\sqrt{\\sigma_X^2} \\]"},{"location":"stats-cheatsheet/#rules-of-variance-standard-deviation","title":"Rules of Variance &amp; Standard Deviation","text":"\\[ Var(aX + bY) = a^2Var(X) + b^2Var(Y) + 2ab Cov(X,Y) \\] \\[ Var(aX - bY) = a^2Var(X) + b^2Var(Y) - 2ab Cov(X,Y) \\] <p>If \\(X\\) and \\(Y\\) are independent,  </p> <p>Standard Deviation,  </p> <p>For any linear function \\(h(X) = aX + b\\),  </p> <p>\\(E[(X - a)^2]\\) is a minumum when  </p> <p>Variance of any constant is zero and if a random variable has zero variance, then it is essentially constant.</p>"},{"location":"stats-cheatsheet/#covariance_1","title":"Covariance","text":"Discrete RVContinuous RV \\[ Cov(X, Y) = \\sigma_{XY} = E[(X - \\mu_X)(Y - \\mu_Y)] = E(XY) - E(X)E(Y) \\] \\[ =\\sum_x\\sum_y(x - \\mu_X)(y - \\mu_Y)p(x,y) = \\biggl(\\sum_x\\sum_y xyp(x,y)\\biggl) - \\mu_X\\mu_Y \\] \\[ =\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x - \\mu_X)(y - \\mu_Y)f(x,y) \\ dx \\ dy =\\biggl(\\int_a^b\\int_c^dxyf(x,y) \\ dx \\ dy \\biggl) - \\mu_X\\mu_Y \\]"},{"location":"stats-cheatsheet/#rules-of-covariance","title":"Rules of Covariance","text":"\\[ Cov(X, X) = E[(X - \\mu_X)^2] = V(X) \\] \\[ Cov(aX + b, cY + d) = acCov(X,Y) \\] \\[ Cov(X, Y+Z) = Cov(X,Y) + Cov(X,Z) \\]"},{"location":"stats-cheatsheet/#correlation_1","title":"Correlation","text":""},{"location":"stats-cheatsheet/#rules-of-correlation","title":"Rules of Correlation","text":"<ul> <li>If \\(a\\) and \\(c\\) are either both positive or both negative,  </li> <li>If \\(ac \\lt 0\\) ,  </li> <li>For any two rv\u2019s X and Y,   </li> </ul>"},{"location":"stats-cheatsheet/#binomial-rv","title":"Binomial RV","text":"MeanVarianceStandard Deviation <p>  where \\(n\\) is the number of trials and \\(p\\) is the probability of success and \\(q\\) is the probability of failure in a single trial.</p> <p>  where \\(q=1-p\\)</p> \\[ \\sigma_x = \\sqrt{np (1 - p)} = \\sqrt{npq} \\]"},{"location":"stats-cheatsheet/#bernoulli-rv","title":"Bernoulli RV","text":"MeanVarianceStandard Deviation \\[ \\mu_X = p = p * 1 + (1 - p) * 0 \\] <p>  where \\(q=1-p\\)</p> \\[ \\sigma_x = \\sqrt{p (1 - p)} = \\sqrt{pq} \\]"},{"location":"stats-cheatsheet/#geometric-rv","title":"Geometric RV","text":"MeanVarianceStandard Deviation \\[ \\mu_X = E(X) = \\frac{1}{p} \\] \\[ \\sigma^2_X = \\frac{1 - p}{p^2} \\] \\[ \\sigma_x = \\sqrt{\\frac{1 - p}{p^2}} \\]"},{"location":"stats-cheatsheet/#distributions","title":"Distributions","text":""},{"location":"stats-cheatsheet/#normal-distribution","title":"Normal Distribution","text":"Probability Density Function (PDF)Cumulative Density Function (CDF) \\[ f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-(x-\\mu)^2/(2 \\sigma^2)} \\] <p>where \\(-\\infty \\lt x \\lt \\infty, -\\infty \\lt \\mu \\lt \\infty, 0 \\lt \\sigma\\)</p> <p>For n independent RVs,</p> \\[ f(x_1,...,x_n;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-(x_1 -\\mu)^2/(2 \\sigma^2)} \\cdot ... \\cdot \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-(x_n -\\mu)^2/(2 \\sigma^2)} \\] \\[ = \\biggl(\\frac{1}{2 \\pi \\sigma^2}\\biggl)^{n/2}e^{-\\sum (x_i-\\mu)^2/(2 \\sigma^2)} \\] \\[ F(x) = P(X \\leq x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\int_{-\\infty}^x e^{-(v-\\mu)^2/2 \\sigma^2} dv \\] \\[ P(a \\leq X \\leq b) = \\int_a^b\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-(x-\\mu)^2/(2 \\sigma^2)}dx \\]"},{"location":"stats-cheatsheet/#standard-normal-distribution-z-distribution-z-sim-nmu0-sigma-1","title":"Standard Normal Distribution (z-Distribution), \\(Z \\sim N(\\mu=0, \\sigma = 1)\\)","text":"Probability Density Function (PDF)Cumulative Density Function (CDF) <p>  where \\(-\\infty \\lt z \\lt \\infty\\)</p> <p>The CDF is obtained as the area under \\(\\phi\\), to the left of \\(z\\).</p> \\[ \\phi(z) = P(Z \\leq z) = \\int_{-\\infty}^z f(y;0,1)dy \\] \\[ =\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^z e^{-u^2/2}du \\] <p>  where the area of the standard normal curve to the right of 0 (between \\(-\\infty\\) to 0) is 1/2.</p> <p>For any \\(c \\gt 0\\),  </p>"},{"location":"stats-cheatsheet/#binomial-distribution","title":"Binomial Distribution","text":"Probability Mass Function (PMF)Cumulative Density Function (CDF) \\[ b(x; n,p) = \\begin{cases} \\binom nxp^x(1-p)^{n-x}, &amp; \\ x=0,1,2,3,...,n \\\\ 0, &amp; \\ otherwise \\end{cases} \\] \\[ B(x; n,p) = P(X \\leq x) = \\sum_{y=0}^xb(y;n,p)  \\ \\  x=0,1,2,...,n \\] <p>** Use cumulative binomial probability table to find out the PMF and CDF values</p> <p>Binomial distribution approaches normal distribution when  </p> \\[ P(X \\leq x) = B(x; n,p) \\approx (area \\ under \\ normal \\ curve \\ to \\ the \\ left \\ of \\ x + .5) \\] <p> provided \\(np \\geq 10\\) and \\(nq \\geq 10\\)</p>"},{"location":"stats-cheatsheet/#bernoulli-distribution","title":"Bernoulli Distribution","text":"Probability Mass Function (PMF)Cumulative Density Function (CDF) <p>If \\(P(X=1)=\\alpha\\), then \\(P(X=0) = 1 - \\alpha\\). Hence  </p> \\[ F(x; \\alpha) = \\begin{cases} 0, &amp; \\  x \\lt 0 \\\\ 1 - \\alpha, &amp; \\  0 \\leq x \\lt 1 \\\\ 1, &amp; \\ x \\geq 1 \\end{cases} \\]"},{"location":"stats-cheatsheet/#geometric-distribution","title":"Geometric Distribution","text":"Probability Mass Function (PMF)Cumulative Density Function (CDF) <p>  where \\(x\\) is the number of failures before the first success</p> <p>  where \\([x]\\) is the largest integer \\(\\leq x\\)</p>"},{"location":"stats-cheatsheet/#poisson-distribution","title":"Poisson Distribution","text":"\\[ p(x; \\mu) = P(X = x) = \\frac{\\mu^x e^{- \\mu}}{x!} \\ \\ x=0,1,2... \\] <p>From Maclaurin series expansion of \\(e^\\mu\\):  </p> <p>Probability that \\(k\\) events will be observed during any particular time interval of length \\(t\\)  where \\(\\mu = \\alpha t\\) and \\(\\alpha\\) is the rate of the event process, the expected number of events occuring in unit time.  Also see Poisson Probability</p> \\[ P(X_1 \\leq t) = 1 - P(X_1 \\gt t) = 1 - e^{-\\alpha t} \\] <p>Also,  </p> <p>And,  </p> Cumulative Density Function (CDF) \\[ F_X(x) = \\frac{\\Gamma(x + 1, \\mu)}{x!} \\] <p>where \\(\\Gamma\\) is the upper incomplete gamma function, a special function that is normally defined in terms of an integral</p> \\[ \\Gamma(s,x) = \\int_x^\\infty t^{s - 1}e^{-t} dt \\] <p></p> MeanVariance \\[ E(X) = \\mu \\] \\[ V(X) = \\mu \\] <p>Poisson distribution approaches normal distribution when  where \\(\\frac{X - \\mu}{\\sqrt\\mu}\\) is the standardized random variable</p>"},{"location":"stats-cheatsheet/#students-t-distribution","title":"Students T-Distribution","text":""},{"location":"stats-cheatsheet/#degrees-of-freedom","title":"Degrees of Freedom","text":"\\[df = n \u2212 1\\] <p>where \\(n\\) is the sample size</p> <p>Also see CI for Difference of Means and CI for Pooled Procedures for T Distribution for degrees of freedom for two sample tests.</p>"},{"location":"stats-cheatsheet/#sampling","title":"Sampling","text":""},{"location":"stats-cheatsheet/#finite-correction-factor-fpc","title":"Finite Correction Factor (FPC)","text":"VarianceStandard Deviation \\[ \\frac{N - n}{N - 1} \\] \\[ \\sqrt \\frac{N - n}{N - 1} \\]"},{"location":"stats-cheatsheet/#sampling-distribution-of-the-sample-meansdsm","title":"Sampling Distribution of the Sample Mean(SDSM)","text":"MeanVarianceStandard DeviationZ-Score \\[ \\mu_{\\overline X} = \\mu_{\\overline x} = \\mu \\] \\[ \\sigma_{\\overline X}^2 = \\sigma_{\\overline x}^2 = \\frac{\\sigma^2}{n} \\approx \\frac{s^2}{n} \\] <p>  This is also called the Standard Error (SE)</p> \\[ z = \\frac{\\overline x - \\mu}{\\frac {\\sigma}{\\sqrt n}} \\]"},{"location":"stats-cheatsheet/#sampling-distribution-of-the-sample-proportionsdsp","title":"Sampling Distribution of the Sample Proportion(SDSP)","text":"MeanVarianceStandard DeviationZ-Score \\[ \\mu_{\\hat p} = p \\] \\[ \\sigma_{\\hat p}^2 = \\frac{p (1 - p)}{n} \\] \\[ \\sigma_{\\hat p} = \\sqrt  \\frac{p (1 - p)}{n} \\] \\[ z_{\\hat p} = \\frac{\\hat p - p}{\\sigma_{\\hat p}} \\]"},{"location":"stats-cheatsheet/#sample-distribution-of-the-difference-of-means-sddm","title":"Sample Distribution of the Difference of Means (SDDM)","text":"MeanStandard Deviation \\[ \\mu_{\\overline x_1 - \\overline x_2} = \\mu_{\\overline x_1} - \\mu_{\\overline x_2} \\] \\[ \\sigma_{\\overline x_1 - \\overline x_2} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}} \\]"},{"location":"stats-cheatsheet/#pooled-procedures","title":"Pooled Procedures","text":"VarianceStandard Deviation \\[ \\sigma_p^2 = \\frac{(n_1 - 1)\\sigma_1^2 + (n_2 - 1)\\sigma_2^2}{n_1 + n_2 - 2} \\] \\[ \\sigma_{\\overline x_1 - \\overline x_2} = \\sigma_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} \\] <p>where  </p>"},{"location":"stats-cheatsheet/#sample-distribution-of-the-difference-of-proportions-sddp","title":"Sample Distribution of the Difference of Proportions (SDDP)","text":"MeanStandard Deviation \\[ \\hat p_1 - \\hat p_2 = \\frac{x_1}{n_1} - \\frac{x_2}{n_2} \\] \\[ \\sigma_{\\hat p_1 - \\hat p_2} = \\sqrt{\\frac{\\hat p_1 (1 - \\hat p_1)}{n_1} + \\frac{\\hat p_2 (1 - \\hat p_2)}{n_2}} \\]"},{"location":"stats-cheatsheet/#confidence-interval","title":"Confidence Interval","text":"<p>  where CL is the Confidence Level</p> Normal DistributionNormal with FPCT Distribution <p>For a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)  where \\(\\overline x\\) is the point estimate of mean \\(\\mu\\)</p> <p>A 100(1 - \\(\\alpha\\))% CI for the \\(\\mu\\) of a normal distribution with standard deviation \\(\\sigma\\) is</p> \\[\\overline x - z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt n} \\lt \\mu \\lt \\overline x + z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt n}\\] \\[ CI = \\overline x \\pm z_ {\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt n} \\sqrt{\\frac{N - n}{N - 1}} \\] \\[ CI = \\overline x \\pm t_{\\alpha, n - 1} \\cdot \\frac{s}{\\sqrt n} \\] <p>Also see Confidence Interval for One and Two Tailed Tests</p> <p>Bound on the error of estimation, B or Margin of Error, ME </p> \\[ \\implies CI = \\overline x \\pm ME \\] <p>Required sample size to estimate \\(\\mu\\) with a \\(100(1 - \\alpha)\\)% confidence and a fixed ME  </p>"},{"location":"stats-cheatsheet/#confidence-interval-for-the-proportion","title":"Confidence Interval for the Proportion","text":"Normal DistributionNormal with FPC \\[ CI = \\hat p \\pm z_ {\\alpha/2} \\cdot \\sqrt {\\frac {\\hat p (1 - \\hat p)}{n}} \\] <p>where \\(\\hat p\\) is the percentage of the subjects that meet the criteria   </p> <p>See SDSP Condition for Inferences</p> \\[ CI = \\hat p \\pm z_ {\\alpha/2} \\cdot \\sqrt {\\frac {\\hat p (1 - \\hat p)}{n}} \\sqrt{\\frac{N - n}{N - 1}} \\] <p>Margin of Error</p> \\[ z_ {\\alpha/2} \\cdot \\sqrt{\\frac {\\hat p (1 - \\hat p)}{n}} \\]"},{"location":"stats-cheatsheet/#confidence-interval-for-difference-of-means","title":"Confidence Interval for Difference of Means","text":"Normal DistributionT Distribution \\[ CI = (\\overline x_1 - \\overline x_2) \\pm z_{\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}} \\] \\[ CI = (\\overline x_1 - \\overline x_2) \\pm t_{\\alpha/2}\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}} \\] <p>with degree of freedom</p> \\[ df = \\frac{\\biggl(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\biggl)^2}{\\frac{1}{n_1 - 1}\\biggl(\\frac{s_1^2}{n_1}\\biggl)^2 + \\frac{1}{n_2 - 1}\\biggl(\\frac{s_2^2}{n_2}\\biggl)^2} \\] <p>  where \\(se_1 = \\frac{s_1}{\\sqrt n_1}, se_2 = \\frac{s_2}{\\sqrt n_2}\\) Note: Round down when degree of freedom is not an integer, so that the estimate is more conservative</p>"},{"location":"stats-cheatsheet/#confidence-interval-for-pooled-procedures","title":"Confidence Interval for Pooled Procedures","text":"T Distribution \\[ CI = (\\overline x_1 - \\overline x_2) \\pm t_{\\alpha/2} * s_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} \\] <p>with degree of freedom  </p>"},{"location":"stats-cheatsheet/#confidence-interval-for-matched-pair-test","title":"Confidence Interval for Matched-Pair Test","text":"Normal DistributionT Distribution <p>  where \\(\\overline d\\) is the mean difference for the matched pair.</p> \\[ CI = \\overline d \\pm t_{\\alpha/2} * \\frac{s_d}{\\sqrt n} \\] <p>with degree of freedom  </p>"},{"location":"stats-cheatsheet/#confidence-interval-for-difference-of-proportions","title":"Confidence Interval for Difference of Proportions","text":"Normal Distribution \\[ CI = (\\hat p_1 - \\hat p_2) \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat p_1 (1 - \\hat p_1)}{n_1} + \\frac{\\hat p_2 (1 - \\hat p_2)}{n_2}} \\]"},{"location":"stats-cheatsheet/#hypothesis-testing","title":"Hypothesis Testing","text":""},{"location":"stats-cheatsheet/#type-1-error-rate","title":"Type 1 Error Rate","text":"\\[ \\alpha=P(rejecting \\ H_0|H_0) \\] \\[ =P(p-value \\leq \\text{significance level} | H_0) \\]"},{"location":"stats-cheatsheet/#power","title":"Power","text":"\\[ 1 - \\beta \\]"},{"location":"stats-cheatsheet/#one-and-two-tailed-tests","title":"One and Two Tailed Tests","text":"Upper Tailed TestLower Tailed TestTwo Tailed Test \\[ H_A: \\mu \\gt \\mu_0 \\] \\[ H_0: \\mu \\leq \\mu_0 \\] \\[ H_A: \\mu \\lt \\mu_0 \\] \\[ H_0: \\mu \\geq \\mu_0 \\] \\[ H_A: \\mu \\neq \\mu_0 \\] \\[ H_0: \\mu = \\mu_0 \\]"},{"location":"stats-cheatsheet/#confidence-interval-for-one-and-two-tailed-tests","title":"Confidence Interval for One and Two Tailed Tests","text":"Upper Tailed TestLower Tailed TestTwo Tailed Test <p>Normal Distribution </p> <p>T Distribution </p> <p>Normal Distribution </p> <p>T Distribution </p> <p>Normal Distribution </p> <p>T Distribution </p>"},{"location":"stats-cheatsheet/#test-statistic","title":"Test Statistic","text":"Normal DistributionT Distribution <p>  where \\(\\mu_0\\) is the Null Hypothesis Mean</p> <p>  where \\(df\\) is the degrees of freedom</p>"},{"location":"stats-cheatsheet/#test-statistic-for-the-proportion","title":"Test Statistic for the Proportion","text":"\\[ z = \\frac{\\hat p - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\]"},{"location":"stats-cheatsheet/#test-statistic-for-difference-of-means","title":"Test Statistic for Difference of Means","text":"Normal DistributionT Distribution \\[ z = \\frac{(\\overline x_1 - \\overline x_2) - (\\mu_1 - \\mu_2)}{\\sqrt {\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\] <p>  with degrees of freedom as shown in CI for Difference of Means </p>"},{"location":"stats-cheatsheet/#test-statistic-for-pooled-procedures","title":"Test Statistic for Pooled Procedures","text":"Normal DistributionT Distribution \\[ z = \\frac{(\\overline x_1 - \\overline x_2) - (\\mu_1 - \\mu_2)}{s_p\\sqrt {\\frac{1}{n_1} + \\frac{1}{n_2}}} \\] <p>  with degrees of freedom as shown in CI for Pooled Procedures</p>"},{"location":"stats-cheatsheet/#test-statistic-for-matched-pair-test","title":"Test Statistic for Matched-Pair Test","text":"Normal DistributionT Distribution <p>  where \\(\\mu_d\\) is the Null Hypothesis Mean Difference</p> \\[ t = \\frac{\\overline d - \\mu_d}{s_d/\\sqrt n}  \\]"},{"location":"stats-cheatsheet/#test-statistic-for-difference-of-proportions","title":"Test Statistic for Difference of Proportions","text":"Normal Distribution \\[ z = \\frac{(\\hat p_1 - \\hat p_2) - (p_1 - p_2)}{\\sqrt {\\hat p (1 - \\hat p)(\\frac{1}{n_1} + \\frac{1}{n_2})}} \\] <p>with   </p> <p>where \\(\\hat p_1 n_1\\) and \\(\\hat p_2 n_2\\) are the number of successes in each sample</p>"},{"location":"stats-cheatsheet/#p-value","title":"p-value","text":"Upper Tailed TestLower Tailed TestTwo Tailed Test <p>The p-value is the area under the standard normal curve to the right of \\(z\\) </p> <p>The p-value is the area under the standard normal curve to the left of \\(z\\) </p> <p>The p-value is sum of the area under the standard normal curve to the left and right of \\(z\\) </p>"},{"location":"stats-cheatsheet/#simple-linear-regression-model","title":"Simple Linear Regression Model","text":"\\[ y = \\beta_0 + \\beta_1 x + u \\] <p>If all other factors (disturbance) are fixed, i.e. if \\(\\Delta u=0\\), then</p> \\[ \\Delta y = \\beta_1 \\Delta x \\] <p>Simplified Form (Equation of Line)</p> \\[ y = \\beta_0 + \\beta_1 x = a + bx \\] <p>Slope</p> \\[ \\beta_1 = b = \\frac{n \\sum {xy} - \\sum x \\sum y}{n \\sum x^2 - (\\sum x)^2} \\] <p>where \\(n\\) is the number of data points</p> <p>Y-Intercept</p> \\[ \\beta_0 = a = \\frac{\\sum y - b \\sum x}{n} \\]"},{"location":"stats-cheatsheet/#errors","title":"Errors","text":"\\[ E(u) = 0 \\] \\[ E(u|x)=E(u) \\implies E(u|x) = 0 \\]"},{"location":"stats-cheatsheet/#residual","title":"Residual","text":"\\[ \\hat u = y_i - \\hat y_i = y_i - \\hat \\beta_0 - \\hat \\beta_1x_i \\] <p>where \\(\\hat u\\), \\(\\hat y_i\\),\\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) are the estimated values. Here \\(\\hat u\\) is different from the error term \\(u\\).</p> <p></p> <p>Residual Properties</p> \\[ \\sum_{i=1}^n \\hat u_i = 0 \\] \\[ \\overline {\\hat u} = 0 \\] \\[ \\sum_{i=1}^n x_i \\hat u_i = 0 \\] \\[ \\overline y = \\hat \\beta_0 + \\hat \\beta_1 \\overline x \\]"},{"location":"stats-cheatsheet/#measures-of-variation","title":"Measures of Variation","text":""},{"location":"stats-cheatsheet/#sum-of-squared-residuals","title":"Sum of Squared Residuals","text":"\\[ \\sum_{i=1}^n \\hat u_i^2 = \\sum_{i=1}^n (y_i - \\hat y_i)^2= \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)^2 \\]"},{"location":"stats-cheatsheet/#explained-sum-of-squares","title":"Explained Sum of Squares","text":"\\[ SSE = \\sum_{i=1}^n(\\hat y_i - \\overline y)^2 \\]"},{"location":"stats-cheatsheet/#residual-sum-of-squares","title":"Residual Sum of Squares","text":"\\[ SSR = \\sum_{i=1}^n \\hat u_i^2  = \\sum_{i=1}^n (y_i - \\hat y_i)^2 \\]"},{"location":"stats-cheatsheet/#total-sum-of-squares","title":"Total Sum of Squares","text":"\\[ SST = \\sum_{i=1}^n(y_i - \\overline y)^2 \\] \\[ SST = SSE + SSR \\]"},{"location":"stats-cheatsheet/#coefficient-of-determination","title":"Coefficient of Determination","text":"\\[ R^2 = 1 - \\frac{SSR}{SST} \\]"},{"location":"stats-cheatsheet/#adjusted-r-squared","title":"Adjusted R-Squared","text":"<p>  where \\(n\\) is the sample size and \\(k\\) is the number of independent variables</p>"},{"location":"stats-cheatsheet/#mean-square-of-regression","title":"Mean Square of Regression","text":"\\[ MSR = \\frac{SSR}{degrees \\ of \\ freedom \\ of \\ SSR} \\]"},{"location":"stats-cheatsheet/#root-mean-square-error","title":"Root Mean Square Error","text":"\\[ RMSE = \\sqrt {\\frac{SSR}{n}} \\]"},{"location":"stats-cheatsheet/#multiple-linear-regression-model","title":"Multiple Linear Regression Model","text":"\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n \\]"},{"location":"stats-cheatsheet/#tolerance","title":"Tolerance","text":"<p>  where \\(R\\) is the coefficient of determination</p>"},{"location":"stats-cheatsheet/#variance-inflation-factor","title":"Variance Inflation Factor","text":"\\[  VIF = \\frac{1}{T} = \\frac{1}{1 \u2013 R^2} \\]"},{"location":"stats-cheatsheet/#durbin-watson-statistic","title":"Durbin Watson Statistic","text":"\\[ d = \\frac{\\sum_{t=2}^T (e_t - e_{t-1})^2}{\\sum_{t=1}^T e_t^2} \\]"},{"location":"stats-cheatsheet/#homoscedasticity","title":"Homoscedasticity","text":"\\[ var(u|x_1,...,x_k) = \\sigma^2\\]"},{"location":"stats-cheatsheet/#polynomial-regression-model","title":"Polynomial Regression Model","text":"\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + ... + \\beta_n x_1^n \\]"},{"location":"stats-cheatsheet/#chi-square-tests","title":"Chi Square Tests","text":"Homogeneity and Association/IndependenceGoodness of Fit <p>Expected Values</p> \\[ Expected_i = \\frac{Observed_{x_{itot}} * Observed_{y_{itot}}}{Observed_{tot}} \\] <p>where \\(Observed_{x_{itot}}\\) and \\(Observed_{y_{itot}}\\) are the total observed counts corresponding to the ith group of the categorical variables \\(x\\) and \\(y\\) respectively and \\(Observed_{tot}\\) is the total count for all variables</p> \\[ \\chi^2 = \\sum{\\frac{(observed - expected)^2}{expected}} \\] <p>Degrees of Freedom </p> <p>Expected Values  where \\(Observed_{tot}\\) is the total count for all variables</p> <p>Degrees of Freedom </p>"},{"location":"stats-cheatsheet/#support-vector-regression-model","title":"Support Vector Regression Model","text":"\\[ \\frac{1}{2}||w||^2 + C \\sum_{i=1}^m (\\xi_i + \\xi_i^*) \\] <p>Minimize</p> \\[ C \\sum_{i=1}^m (\\xi_i + \\xi_i^*) \\]"},{"location":"stats-cheatsheet/#logistic-regression-model","title":"Logistic Regression Model","text":"\\[ logit(p_i) = ln\\biggl(\\frac{p_i}{1 - p_i}\\biggl) = \\beta_0 + \\beta_1x_{1,i}+\\dots+\\beta_Mx_{m,i} \\] <p>where \\(p_i\\) is the probability \\(\\frac{p_i}{1 - p_i}\\) is the odds of success and \\(ln \\frac{p_i}{1 - p_i}\\) is the log of the odds of success</p> <p></p> <p>Sigmoid Function</p> <p>Solving the above for p, we get   where \\(z = \\beta_0 + \\beta_1x_1+ \\dots\\)</p>"},{"location":"stats-cheatsheet/#likelihood-function","title":"Likelihood Function","text":"<p>Intuition </p> <p>Also see <code>Mean</code> tab for Binomial RV and Bernoulli RV</p> <p>Likelihood for samples labelled as <code>1</code>:  </p> <p>where \\(x_i\\) represents the feature vector for the \\(i^{th}\\) sample</p> <p>Likelihood for samples labelled as <code>0</code>:  </p> <p></p> <p>Overall Likelihood:  </p> <p>Log Likelihood:  </p>"},{"location":"stats-cheatsheet/#kmeans-clustering","title":"KMeans Clustering","text":""},{"location":"stats-cheatsheet/#wcss","title":"WCSS","text":"<p>  where \\(P_{i1}\\) is the \\(i^{th}\\) point in cluster 1, \\(C_1\\) is the center of cluster 1, \\(m\\) is the number of points in a cluster, \\(n\\) is the number of clusters and distance is the Euclidean distance between a point and the center of the cluster</p>"},{"location":"stats-cls/","title":"Classification","text":"<p>Classification is the process of estimating the value of the dependent categorical variable from the independent variable(s).</p>"},{"location":"stats-cls/#logistic-regression","title":"Logistic Regression","text":"<ul> <li>Predict a categorical dependent variable from a set of independent variables</li> <li>Curve is defined by the sigmoid function</li> <li>Parameters are estimated using the Maximum Likelihood Estimation</li> </ul>"},{"location":"stats-cls/#likelihood-function","title":"Likelihood Function","text":"<ul> <li>To estimate \\(\\beta\\) vectors consider a sample of data points with labels either 0 or 1</li> <li>Projection of data points on the sigmoid curve gives the probability of success (probability of the point being labelled as 1) for the data point<ul> <li>Likelihood for samples labeled as <code>1</code><ul> <li>we try to estimate \\(\\beta\\) such that the product of probability of all data points is as close to 1 as possible</li> <li>the projection of the data point on the sigmoid curve gives the probability of success, so \\(p(x)\\) gives the probability of being close to 1</li> </ul> </li> <li>Likelihood for samples labeled as <code>0</code><ul> <li>we try to estimate \\(\\beta\\) such that the product of probability of all data points is as close to 0 as possible </li> <li>the projection of the data point on the sigmoid curve gives the probability of success, so \\(1 - p(x)\\) gives the probability of being close to 0</li> </ul> </li> <li>The likelihood function returns the probability of how well the model(curve) predicts the label<ul> <li>Combination (Product) of conditions for samples labeled as <code>1</code> and <code>0</code></li> </ul> </li> </ul> </li> <li>The likelihood function across multiple models (sigmoid curves) is optimized to get the maximum likelihood</li> </ul>"},{"location":"stats-cls/#k-nearest-neighbor","title":"K-Nearest Neighbor","text":"<ul> <li>Steps<ul> <li>Choose number of neighbors (K) - usually an odd number</li> <li>Take the K nearest neighbors measured by the Euclidean distance (most common distance calculation used)</li> <li>Count the number of data points in each category among the neighbors</li> <li>Predict dependent data point to be in the category with the most neighbors</li> </ul> </li> </ul>"},{"location":"stats-cls/#support-vector-machine-svm","title":"Support Vector Machine (SVM)","text":"<ul> <li>Helps deine the best decision boundary to separate the categories</li> <li>Determines the plane for the Maximum Margin Hyperplane (Maximum Margin Classifier)<ul> <li>Equidistant from the Support Vectors</li> <li>Support Vectors are the vectors that are closest to the Maximum Margin Hyperplane<ul> <li>Think of these as the vectors which are the most similar (most challenging to differentiate) across the categories</li> </ul> </li> </ul> </li> <li>Objective is to maximize the sum of the distances of the Maximum Margin plane from the support vectors</li> </ul>"},{"location":"stats-cls/#using-kernels","title":"Using Kernels","text":"<ul> <li>Mapping non linear data points to higher planes allows us to define linear boundaries to separate them<ul> <li>We can then project them back to the original plane to get the non linear separator in the original plane</li> </ul> </li> <li>Kernels help avoid the high compute intensive requirements that comes with mapping data points to higher planes</li> </ul>"},{"location":"stats-cls/#naive-bayes","title":"Naive Bayes","text":"<ul> <li>Assumes the dependent variables to be independent ( and are not correlated)</li> </ul>"},{"location":"stats-cls/#decision-tree-classification","title":"Decision Tree Classification","text":"<ul> <li>Also See Decision Tree Regression</li> <li>Uses entropy for splitting data</li> </ul>"},{"location":"stats-clustr/","title":"Clustering","text":"<p>Clustering is a set of data reduction techniques which are designed to group similar observations in a dataset, such that observations in the same group are as similar to each other as possible, and observations in different groups are as different to each other as possible</p>"},{"location":"stats-clustr/#k-means-clustering","title":"K-Means Clustering","text":"<ul> <li>Decide how many clusters<ul> <li>Can use Elbow Method to find optimal number of clusters<ul> <li>Run the K-Means clustering first to get the number of clusters</li> <li>Calculate WCSS for each cluster combination</li> <li>Plot WCSS against the number of clusters</li> <li>Find the elbow in the WCSS plot to determine the Optimal number of clusters</li> </ul> </li> </ul> </li> <li>Assign a random center for each of the clusters<ul> <li>May result in the Random Initialization Trap</li> </ul> </li> <li>Group based on the above centers</li> <li>Recalculate the centers for each of the new groups<ul> <li>Regroup and repeat the above step till the groups stay the same (centers do not change)</li> </ul> </li> </ul>"},{"location":"stats-clustr/#k-means-clustering_1","title":"K-Means++ Clustering","text":"<ul> <li>Initial selection of cluster centers is based on an algorithm<ul> <li>First center is selected uniformly at random among the data points</li> <li>For each of the remaning data points calculate the distance, \\(D\\) to the nearest of the already selected centers</li> <li>Choose one new data point at random as a new center, using a weighted probability distribution where a point \\(x\\) is chosen with probability proportional to \\(D(x)^2\\)</li> <li>Repeat till number of desired centers have been selected </li> <li>Proceed using standard K-Means clustering</li> </ul> </li> <li>Helps avoiding the Random Initialization Trap</li> </ul>"},{"location":"stats-distributions/","title":"Distributions","text":""},{"location":"stats-distributions/#normal-distribution-gaussian-distribution-x-sim-nmu-sigma2","title":"Normal distribution (Gaussian Distribution), \\(X \\sim N(\\mu, \\sigma^2)\\)","text":"<p>A symmetric, bell-shaped distribution for Continuous Random Variables that models measurement error, intelligence/ability, height, averages of lots of data</p> <p></p> <p></p> <p>Tip</p> <p>The probability of a random variable falling within </p> <ul> <li>1 Standard Deviation is 68.2% i.e., 68.2% of the normal distribution area is between standard deviation -1 and 1</li> <li>2 Standard Deviations is 95.4% i.e., 95.4% of the normal distribution area is between standard deviation -2 and 2</li> <li>3 Standard Deviations is 99.7% i.e., 99.7% of the normal distribution area is between standard deviation -3 and 3</li> </ul> <p>Also see z-score tip and Z-Values for Common Confidence Levels</p> <p>Symmetric distribution: The distribution\u2019s mean and median are at the very center of the distribution, with an equal about of data to the left and right</p> <p>Skewed distribution: A non-symmetric distribution that leans right or left. </p> <p>Note</p> <ul> <li>Negatively/left-skewed/left-tailed distributions have their tail on the left<ul> <li>Mean &lt; Median &lt; Mode</li> </ul> </li> <li>Positively/right-skewed/right-tailed distributions have their tail on the right<ul> <li>Mean &gt; Median &gt; Mode</li> </ul> </li> </ul>"},{"location":"stats-distributions/#outliers","title":"Outliers","text":"<p>Expression</p> <p>Low outliers: \\(Q1 - 1.5(IQR)\\)</p> <p>High outliers: \\(Q3 + 1.5(IQR)\\)</p> <p>When we have a data set with outliers that skew the data,</p> <ul> <li>the median will be a better measure of central tendency than the mean</li> <li>the interquartile range will be a better measure of spread than standard deviation</li> </ul> <p>That\u2019s because mean and standard deviation will take into account all points in the data set, including the outliers. But median and IQR can ignore these outliers, giving us more accurate measurements of the data. </p>"},{"location":"stats-distributions/#z-scores","title":"Z-Scores","text":"<p>The z-score for a data point x is the score that tells us the number of standard deviations between between x and the mean of \u03bc. </p> <ul> <li>Use z-score to find out the probability(p-value) of finding a random point in a normal distribution. </li> <li>Use z score tables (also known as Standard normal curve table) to find the probability corresponding to a given z score. <ul> <li>The probability values in the table represent the probability of finding the point in the entire area to the left of the point. </li> <li>In order to find the probability between the mean (which is 0 for normal distribution and will be at the 50% mark) and the point, subtract 50 from the probability value shown in the table.</li> <li>Data points that are less than the mean will be to the left of the mean and will have a negative z-score. They should be looked up in the table of negative z-scores </li> <li>Data points that are greater than the mean will be to the right of the mean and will have a positive z-score. They should be looked up in the table of positive z-scores </li> <li>The z-table always gives us the percentage of data that is below our data point. Therefore, to find the percentage of data above our data point, we have to take 1 minus the value from the table</li> <li>z score tables can also be used to find the z row and column values (z critical value) corresponding to the percentile</li> </ul> </li> </ul>"},{"location":"stats-distributions/#critical-values","title":"Critical Values","text":"<p>The z-critical value, \\(z_\\alpha\\) is the \\(100(1 - \\alpha)\\)th percentile of the standard normal distribution</p> <ul> <li>We can use qnorm to find the z critical values : \\(z_{0.05} = qnorm(0.95, 0,1)\\) </li> <li> <p>For finding the z-critical value corresponding to \\(\\alpha\\) = 5 find the row and column corresponding to .95 (this will be equal to 1.64)</p> <p><p></p></p> <p>Tip</p> <ul> <li>95% of the area of standard normal distribution is within (to the left of) \\(Z = 1.64\\)</li> <li>99.5% of the area of standard normal distribution is within (to the left of) \\(Z = 2.58\\)</li> <li>68% of the area is within 1 standard deviation of the mean, \\(P(-1 \\leq Z \\leq 1) = .6826\\)</li> <li>95% is within 2 standard deviations, \\(P(-2 \\leq Z \\leq 2) \\approx .95\\) ; more precisely \\(P(-1.96 \\leq Z \\leq 1.96) = .95\\)</li> <li>99% is within \\(P(-2.58 \\leq Z \\leq 2.58)\\) ; more precisely \\(P(-2.58 \\leq Z \\leq 2.58) = .9902\\)</li> <li>99.7% is within 3 standard deviations, \\(P(-3 \\leq Z \\leq 3) \\approx .997\\) ; more precisely \\(P(-3.29 \\leq Z \\leq 3.29) = .999\\)</li> <li>\\(P(Z \\lt 1) \\approx 0.8413\\)</li> <li>\\(P(Z \\lt 2) \\approx 0.9772\\)</li> <li>\\(P(Z \\lt 3) \\approx 0.9987\\)</li> </ul> <p>Also see normal distribution tip and Z-Values for Common Confidence Levels</p> </li> <li> <p>Use pnorm with z value as parameter to get the corresponding percentile and use qnorm with the percentile to get the corresponding z value</p> </li> <li>If the z-critical value falls in the rejection region, we reject the Null Hypothesis<ul> <li>Also see Rejecting Null Hypothesis Based on critical values</li> </ul> </li> <li>z-critical value, \\(z_ {\\alpha/2}\\) for the upper tail is the right critical value (\\(P(Z \\gt \\alpha/2) = \\alpha/2\\)) and that for the lower tail is the left critical value (\\(P(Z \\lt \\alpha/2) = - \\alpha/2)\\)<ul> <li>\\(\\pm z_ {\\alpha/2}\\) are also referred to as the boundaries of the region of rejection</li> </ul> </li> </ul>"},{"location":"stats-distributions/#chebyshevs-theorem","title":"Chebyshev\u2019s Theorem","text":"<p>For all \\(k \\gt 1\\), at least \\((1 - \\frac{1}{k^2})\\)% of our data must be within \\(k\\) standard deviations of the mean, regardless of the shape of the data\u2019s distribution.  </p> <p>Info</p> <ul> <li>At least 75 % of the data must be within k = 2 standard deviations of the mean. </li> <li>At least 89 % of the data must be within k = 3 standard deviations of the mean. </li> <li>At least 94 % of the data must be within k = 4 standard deviations of the mean.</li> </ul>"},{"location":"stats-distributions/#binomial-distribution-x-sim-binnp","title":"Binomial Distribution, \\(X \\sim Bin(n,p)\\)","text":"<p>This type of distribution is based on Binomial Variables (Discrete RV)</p> <p>Suppose we are repeating an experiment having only two outcomes \\(\\{\\text{success},\\text{failure}\\}\\), \\(n\\) times in such are way that their outcomes are independent where \\(p = P(\\text{success})\\), then the probability of \\(x\\) successes in \\(n\\) trials is given by the Binomial Probability</p>"},{"location":"stats-distributions/#geometric-distribution","title":"Geometric Distribution","text":"<p>This type of distribution is based on Geometric Variables  (Discrete RV)</p> <p>Also see Geometric Probability.</p>"},{"location":"stats-distributions/#poisson-distribution","title":"Poisson Distribution","text":"<p>Poisson process: Calculates the number of times an event occurs in a period of time, or in a particular area, or over some distance, or within any other kind of measurement.</p> <ul> <li>The experiment counts the number of occurrences of an event over some other measurement,</li> <li>The mean is the same for each interval,</li> <li>The count of events in each interval is independent of the other intervals,</li> <li>The intervals don\u2019t overlap, and</li> <li>The probability of the event occurring is proportional to the period of time.</li> <li>Used to model Discrete RVs</li> </ul> <p>The probability that \\(k\\) events will be observed during any particular time interval of length \\(t\\) is given by the Poisson Probability</p>"},{"location":"stats-distributions/#students-t-distribution","title":"Students T-Distribution","text":"<ul> <li>Similar to the standard normal distribution in the sense that it\u2019s <ul> <li>symmetrical</li> <li>bell-shaped</li> <li>and centered around the mean \u03bc = 0, </li> </ul> </li> <li>Heavier tails (flatter and wider distribution compared to standard normal distribution)<ul> <li>The larger the sample size, the taller and narrower the t-distribution gets. </li> <li>The smaller the sample size, the shorter and wider the t-distribution gets. </li> </ul> </li> <li>Standard Deviation of the T-Distribution is larger than that of the Standard Normal Distribution</li> <li>The exact shape depends on the number of degrees of freedom</li> <li>Smaller sample sizes <ul> <li>Becomes similar to normal distribution as sample size increases and approaches infinity (~ 30 degrees of freedom)</li> <li>Used when sample size is less than 30</li> </ul> </li> <li>Harder to reject the null hypothesis</li> <li>Use t score table to find the t critical value corresponding to a given t score<ul> <li>In the t-table, we see values for upper-tail probability along the top of the table, and confidence levels along the bottom of the table</li> </ul> </li> </ul> <p>Upper Tail Probabilities and Confidence Levels</p> <ul> <li>Upper Tail Probability of 0.05    = Confidence Level of 90%</li> <li>Upper Tail Probability of 0.025   = Confidence Level of 95%</li> <li>Upper Tail Probability of 0.01    = Confidence Level of 98%</li> <li>Upper Tail Probability of 0.005   = Confidence Level of 99%</li> </ul>"},{"location":"stats-distributions/#confidence-intervals-ci","title":"Confidence Intervals (CI)","text":"<p>Point estimate: An estimator for a singular value, like the sample mean for the population mean, or the sample standard deviation for the population standard deviation</p> <p>Interval estimate: A range of values that estimate the interval in with some parameter may lie</p> <p>Confidence level: The probability that an interval estimate will include the population parameter</p> <p>Alpha Value: The probability of making a Type 1 Error. It is also called the Level of Significance</p> <ul> <li>A (\\(1 - \\alpha\\)) CI has a significance level of \\(\\alpha\\)</li> <li>\\(\\alpha\\) is split evenly between the upper and lower tails (\\(\\frac {\\alpha}{2}\\))</li> <li>The region under \\(\\alpha\\) is referred to as the Rejection region <ul> <li>It is the region outside the confidence interval, in the tail(s) of the probability distribution.</li> </ul> </li> </ul> <p>Confidence Interval can be denoted as the point estimate of \\(\\mu \\pm (z\\) critical value) (standard error of the mean)</p> <p></p> <p>Z-Values for Common Confidence Levels</p> <ul> <li>For a 90% CL, \\(\\alpha\\) = .10, \\(\\alpha/2\\) = .05, \\(z_{\\frac{\\alpha}{2}} = \\pm 1.65\\)</li> <li>For a 95% CL, \\(\\alpha\\) = .05, \\(\\alpha/2\\) = .025, \\(z_{\\frac{\\alpha}{2}} = \\pm 1.96\\)</li> <li>For a 99% CL, \\(\\alpha\\) = .01, \\(\\alpha/2\\) = .005, \\(z_{\\frac{\\alpha}{2}} = \\pm 2.58\\)</li> </ul> <p>Also see z-score tip and normal distribution tip</p> <p>Note</p> <ul> <li>The higher the confidence level, the wider the confidence interval (because as \\(z\\) gets larger, the ME will get larger which makes the confidence interval wider)</li> <li>The larger the population standard deviation \\(\\sigma\\), the wider the confidence interval (because as \\(\\sigma\\) gets larger, the ME will get larger which makes the confidence interval wider)</li> <li>The larger the sample size \\(n\\), the narrower the confidence interval (because as \\(n\\) gets larger, the ME will get smaller which makes the confidence interval narrower)</li> </ul> <p>Remember</p> <p>We want the smallest confidence interval we can get because the smaller the confidence interval, the more accurately we can estimate the population parameter</p>"},{"location":"stats-hypo-test/","title":"Hypothesis Testing","text":""},{"location":"stats-hypo-test/#concepts","title":"Concepts","text":"<p>Hypothesis Testing Process</p> <ol> <li>State the null and alternative hypotheses.</li> <li>Determine the level of significance, \\(\\alpha\\).</li> <li>Set up the decision rule (type of test and test statistic, \\(z\\) or \\(t\\), to be used)</li> <li>Calculate the test statistic.</li> <li>Find critical value(s) and determine the regions of acceptance and rejection.</li> <li>State the conclusion.</li> </ol> <p>Alternative hypothesis: The abnormality we\u2019re looking for in the data; it\u2019s the significance we\u2019re hoping to find</p> <p>Null hypothesis: The opposite claim of the alternative hypothesis</p> <p>Remember</p> <p>The objective of Hypothesis Testing is to find enough evidence to reject the Null Hypothesis \\(H_0\\). </p> <ul> <li>If \\(H_0\\) can be confidently rejected, then the Alternative Hypothesis \\(H_a\\) is true. </li> <li>If \\(H_0\\) cannot be confidently rejected, then \\(H_a\\) may or may not be true. </li> </ul>"},{"location":"stats-hypo-test/#type-i-and-type-ii-errors","title":"Type I and Type II errors","text":"<p>Type I error: </p> <ul> <li>When we mistakenly REJECT a null hypothesis that\u2019s actually true. <ul> <li>The probability of making a Type I error is given by Alpha \\(\\alpha\\)</li> </ul> </li> <li>In Machine Learning, this happens when the model incorrectly predicts a positive result (1) although the actual value is negative (0)<ul> <li>This is also referred to as False Positives</li> </ul> </li> </ul> <p>Type II error: </p> <ul> <li>When we mistakenly DO NOT REJECT a null hypothesis that\u2019s actually false. <ul> <li>The probability of making a Type II error is given by Beta \\(\\beta\\).</li> </ul> </li> <li>In Machine Learning, this happens when the model incorrectly predicts a negative result (0) although the actual value is positive (1)<ul> <li>This is also referred to as False Negatives</li> </ul> </li> </ul> <p>Power: The probability that we\u2019ll reject the null hypothesis when it\u2019s false. </p> <ul> <li>We want our test to have a high power</li> </ul> \\(H_0\\) is True \\(H_0\\) is False Reject \\(H_0\\) Type I Error P(Type I Error) = \\(\\alpha\\) CORRECT  Power Do Not Reject \\(H_0\\) CORRECT Type II Error P(Type II Error) = \\(\\beta\\)"},{"location":"stats-hypo-test/#one-and-two-tailed-tests","title":"One and Two Tailed Tests","text":"<p>One Tailed Test (One-Sided or Direction Test)</p> <ul> <li>Upper-tailed test, right-tailed test: The alternative hypothesis states that one value is greater than another, while the null hypothesis states that one value is less than or equal to the other<ul> <li>\\(z\\) is positive</li> </ul> </li> <li>Lower-tailed test, left-tailed test: The alternative hypothesis states that one value is less than another, while the null hypothesis states that one value is greater than or equal to the other<ul> <li>\\(z\\) is negative</li> </ul> </li> <li>Has a larger rejection region, as the entire rejection region is consolidated into one tail</li> <li>Used when we are confident about directionality and it does not make sense to run the test in both directions</li> <li>enable more Type I errors (false positives) and also cognitive bias errors</li> </ul> <p>Two Tailed Test (Two-Sided or Non-Directional Test) - The alternative hypothesis states that one value is unequal to another, while the null hypothesis states that one value is equal to the other</p> <ul> <li>We do not predict any direction between the variables<ul> <li>Not trying to predict whether one value is greater or less than the other</li> </ul> </li> <li>There are two region of rejections - one in each tail</li> <li>Always more conservative than one tailed tests</li> <li>Used when we are not confident about the directionality</li> </ul>"},{"location":"stats-hypo-test/#test-statistic","title":"Test Statistic","text":"\\[ Test \\ Statistic = \\frac{Observed - Expected}{Standard \\ Error} \\]"},{"location":"stats-hypo-test/#p-value-observed-level-of-significance","title":"p-value (Observed Level of Significance)","text":"<p>The smallest level of significance at which we can reject the null hypothesis, assuming the null hypothesis is true. </p> <ul> <li>It is the probability for the total area of the region of rejection</li> <li>Because a p-value is a probability, its value is always between zero and one </li> </ul> <p>Remember</p> <p>The smaller the p-value, the more significant the result</p>"},{"location":"stats-hypo-test/#rejecting-null-hypothesis","title":"Rejecting Null Hypothesis","text":"<p>Based on p-value</p> <ul> <li>If \\(p \\leq \\alpha\\), reject the null hypothesis</li> <li>If \\(p \\gt \\alpha\\), do not reject the null hypothesis</li> </ul> <p></p> <p>Based on  critical value</p> <ul> <li>Lower Tailed Test: Reject \\(H_0\\) when \\(z \\leq - z_\\alpha\\)</li> <li>Upper Tailed Test: Reject \\(H_0\\) when \\(z \\geq z_\\alpha\\)</li> <li>Two Tailed Test: Reject \\(H_0\\) when \\(z \\leq - z_{\\alpha/2}\\) or \\(z \\geq z_{\\alpha/2}\\)</li> </ul> <p>Let \\((\\hat \\theta_L, \\hat \\theta_U)\\) be a confidence interval for \\(\\theta\\) with confidence level \\(100(1 \u2014\\alpha)\\)%. Then a test of \\(H_0: \\theta = \\theta_0\\) versus \\(H_A: \\theta \\neq \\theta_0\\) with significance level \\(\\alpha\\) rejects the null hypothesis if the null value \\(\\theta_0\\) is not included in the \\(CI\\) and does not reject \\(H_0\\) if the null value does lie in the \\(CI\\).</p>"},{"location":"stats-reg/","title":"Regression","text":""},{"location":"stats-reg/#simple-linear-regression-model","title":"Simple Linear Regression Model","text":"<p>Regression is the process of estimating the value of the dependent variable from the independent variable(s).</p> <p>The most common form of the Regression Model is the Simple Linear Regression Model</p> <ul> <li>Also called two-variable linear regression model or bivariate linear regression model</li> </ul> <p>It is represented as the equation of the line that best shows the trend in the data. The line is referred to as the regression line.</p> <ul> <li>Also called line of best fit or the least squares line (see Sum of Squared Residuals)</li> </ul> <p>Expression</p> \\[y=\\beta_0 + \\beta_1 x + u\\] <p>where </p> <ul> <li>\\(y\\) is the dependent variable (also called explained, outcome, predicted or response variable or just regressand),</li> <li>\\(\\beta_0\\) is the intercept,</li> <li>\\(\\beta_1\\) is the slope parameter,</li> <li>\\(x\\) is the independent variable (also called control, predictor or explanatory variable or just regressor or covariate) and</li> <li> <p>\\(u\\) is the error term or disturbance and includes the effect of all other factors aside from \\(x\\)</p> <ul> <li>It is the difference between the actual value of a data point and the predicted value (value of the point on the regression line) and is also called the residual for that data point</li> </ul> </li> <li> <p>\\(\\beta_0 + \\beta_1 x\\) is referred to as the systematic part of \\(y\\), the part that can be expalined by \\(x\\) </p> </li> <li>\\(u\\) is the unsystematic part of \\(y\\), the part that cannot be expalined by \\(x\\)</li> </ul> <p>If the regression line has a</p> <ul> <li>positive slope, the data has a positive linear relationship</li> <li>negative slope, the data has a negative linear relationship</li> </ul> <p>Also see Correlation Basics Tip</p> <p>If the data is clustered</p> <ul> <li>tightly around the regression line, it shows a strong relationship</li> <li>loosely around the regression line, it shows a moderate or weak relationship depending on the spread of the data points<ul> <li>the more outliers there are in the data, the weaker the relationship</li> </ul> </li> </ul> <p>Info</p> <p>In statistics, we usually write the slope and intercept at least to four decimal places to prevent severe rounding errors and getting a more accurate regression line</p> <p>Error Assumptions</p> <ul> <li>Errors have zero means, \\(E(u) = 0\\)</li> <li>Average value of \\(u\\) does not depend on value of \\(x\\), i.e. \\(u\\) is mean independent of x. This is referred to as the zero conditional mean assumption , \\(E(u|x)=E(u) \\implies E(u|x) = 0\\)</li> </ul> <p>Residual Properties</p> <ul> <li>Estimated errors sum up to zero, \\(\\sum_{i=1}^n \\hat u_i = 0\\)</li> <li>For any regression line, the mean of residuals is always zero, \\(\\overline {\\hat u} = 0\\)</li> <li>Correlation between residuals and regressors is zero, \\(\\sum_{i=1}^n x_i \\hat u_i = 0\\)</li> <li>Sample averages of y and x (point \\(\\overline x, \\overline y\\)) lie on a regression line, \\(\\overline y = \\hat \\beta_0 + \\hat \\beta_1 \\overline x\\)</li> </ul>"},{"location":"stats-reg/#multiple-linear-regression-model","title":"Multiple Linear Regression Model","text":""},{"location":"stats-reg/#assumptions","title":"Assumptions","text":"<ul> <li>Linearity - The relationship between the independent and dependent variables should be linear.  <ul> <li>It is also important to check for outliers since linear regression is sensitive to outlier effects</li> <li>The linearity assumption can best be tested with scatter plots</li> </ul> <p>Remember</p> <p>Always analyze the data distribution in order to determine if Linear Regression can be used to model the data. See Anscombe's Quartet</p> </li> <li>Multivariate Normality - All variables should be multivariate normal, i.e. the data points should be normally distributed along the line of linear regression.  <ul> <li>This assumption can best be checked with a histogram or a Q-Q-Plot</li> <li>Normality can be checked with a goodness of fit test, e.g., the Kolmogorov-Smirnov test</li> <li>When the data is not normally distributed, a non-linear transformation (e.g., log-transformation) might fix this issue</li> </ul> </li> <li>Lack of MultiCollinearity - There should be little or no multicollinearity in the data, i.e the predictors (independent variables) are not correlated to each other.<ul> <li>Multicollinearity may be tested with three central criteria:<ul> <li>Correlation matrix \u2013 when computing the matrix of Pearson\u2019s Bivariate Correlation among all independent variables the correlation coefficients need to be smaller than 1</li> <li>Tolerance \u2013 the tolerance measures the influence of one independent variable on all other independent variables  </li> </ul> <p>Tip</p> <ul> <li>With T &lt; 0.1 there might be multicollinearity in the data </li> <li>With T &lt; 0.01 there  is multicollinearity in the data</li> </ul> <ul> <li>Variance Inflation Factor (VIF) - This is good indicator when the sample size is small but is of limited use for large sample sizes</li> </ul> <p>Tip</p> <ul> <li>With VIF &gt; 5 there is an indication that multicollinearity may be present</li> <li>With VIF &gt; 10 there is certainly multicollinearity among the variables </li> </ul> <ul> <li>Condition Index \u2013 the condition index is calculated using a factor analysis on the independent variables.  <ul> <li>Values of 10-30 indicate a mediocre multicollinearity</li> <li>Values &gt; 30 indicate strong multicollinearity</li> </ul> </li> </ul> </li> <li>If multicollinearity is found in the data<ul> <li>Centering the data (deducting the mean of the variable from each score) might help to solve the problem</li> </ul> </li> </ul> </li> <li>Independence (Autocorrelation or Serial Correlation) - There should be little or no autocorrelation in the data.  Autocorrelation occurs when the residuals/rows are not independent from each other<ul> <li>occurs when the value of current row y(x) is not independent from the value of the previous row y(x-1)</li> <li>common for time series data</li> <li>can be tested with the Durbin-Watson test<ul> <li>Durbin-Watson\u2019s d tests the null hypothesis that the residuals are not linearly auto-correlated.  <ul> <li>d can assume values between 0 and 4</li> <li>values of 1.5 &lt; d &lt; 2.5 show that there is no auto-correlation in the data</li> <li>only analyses linear autocorrelation and only between direct neighbors(successive data points), which are first order effects</li> </ul> </li> </ul> </li> </ul> </li> <li>Homoscedasticity - data should be homoscedastic, i.e. the variance in the error term, \\(u\\), is the same for all combinations of outcomes of the independent variables.  <ul> <li>Scatter plot should not show cone shaped distribution</li> </ul> </li> </ul>"},{"location":"stats-reg/#modeling-methods","title":"Modeling Methods","text":"<ul> <li>Use all variables<ul> <li>Not preferred unless there is a compelling reason</li> </ul> </li> <li> <p>Backward Elimination</p> <ul> <li>Select significance level (e.g. \\(\\alpha\\) = 0.05)</li> <li>Fit model with all variables</li> <li>If the variable with the highest p-value is greater than \\(\\alpha\\)<ul> <li>Remove the variable </li> <li>Fit the model again with the remaining variables</li> <li>Repeat process till the highest p-value is no longer greater than \\(\\alpha\\)</li> </ul> </li> </ul> </li> <li> <p>Forward Selection</p> <ul> <li>Select significance level (e.g. \\(\\alpha\\) = 0.05)</li> <li>Create separate Simple Linear Regression models by fitting each variable separately</li> <li>Select the model with the lowest p-value<ul> <li>Create separate Regression models by fitting each of the remaining variables separately to this model</li> <li>Repeat process till the lowest p-value is no longer lesser than \\(\\alpha\\)</li> <li>Use the model prior to the one that resulted in p-value greater than \\(\\alpha\\)</li> </ul> </li> </ul> </li> <li> <p>Bidirectional Elimination (Stepwise Regression)</p> <ul> <li>Select significance levels to enter and stay in the model - both can be same (e.g. \\(\\alpha\\) = 0.05)</li> <li>Perform Forward Selection step<ul> <li>Select the model with the lowest p-value </li> <li>Perform Backward Selection step with remaining variables</li> <li>Remove the variables where p-value is greater than \\(\\alpha\\)</li> <li>Create separate Regression models by fitting each of the remaining variables separately to the Forward Selection model</li> <li>Repeat the process till there are no variables to enter or exit</li> </ul> </li> </ul> </li> <li> <p>Score Comparison</p> <ul> <li>Select a criterion of goodness of fit (e.g. Akaike criterion)</li> <li>Construct all possible regression models (discusssed above)<ul> <li>\\(2^N - 1\\) total models can be created</li> </ul> </li> <li>Select the model with the best criterion</li> </ul> </li> </ul> <p>When writing python code, we usually don't have to worry about using a certain method for selecting the most statistically significant features. This is taken care of by the regression class in scikit learn package automatically.</p>"},{"location":"stats-reg/#polynomial-regression-model","title":"Polynomial Regression Model","text":"<p>The polynomial regression model is a linear regression even though the variables are not linear (they grow expnentially) as the linearity in the regression model is based on the coefficients of the independent variables(which are linear in this case).</p>"},{"location":"stats-reg/#chi-square-tests","title":"Chi Square Tests","text":"<p>Helps investigate the relationship betweem categorical variables. Refer Chi Square Distribution Table</p> <p>Conditions for Inference</p> <ul> <li>Random<ul> <li>Sample should be random</li> </ul> </li> <li>Large Counts<ul> <li>Each expected value should be 5 or greater</li> </ul> </li> <li>Independent<ul> <li>Sample with replacement AND/OR</li> <li>\\(n \\leq \\frac{N}{10}\\) (Sample size less than 10% of total population - 10% Rule)</li> </ul> </li> <li>Categorical<ul> <li>variables should be categorical</li> </ul> </li> </ul> <p>Three types:</p> <ul> <li>\\(\\chi^2\\) test for homogeneity<ul> <li>Whether the probability distributions of two separate groups are homogenous (similar) with respect to some characteristic<ul> <li>The values corresonding to a homogenous distribution are referred to as the expected values</li> <li>The larger the value of \\(\\chi^2\\), the more likely that the variables affect each other and are not homogenous</li> <li>Compare the calculated \\(\\chi^2\\) against the chi square table value based on the desired \\(\\alpha\\) and degree of freedom to test the null hypothesis</li> <li>Reject null hypothesis if \\(\\chi^2 \\gt \\chi_\\alpha^2\\)</li> </ul> </li> </ul> </li> <li>\\(\\chi^2\\) test for association/independence<ul> <li>Whether two variables in the same group are related</li> </ul> </li> <li>\\(\\chi^2\\) test for goodness of fit<ul> <li>Whether data (in a single data table) fits a specified distribution</li> </ul> </li> </ul>"},{"location":"stats-reg/#decision-tree-regression","title":"Decision Tree Regression","text":"<ul> <li>At a very high level,<ul> <li>Splits data to maximize categories and finding optimal splits</li> <li>The splitting process is repeated till all optimal splits are found<ul> <li>The decision process for splitting data results in a tree like structure</li> </ul> </li> <li>A split is also referred to as a leaf<ul> <li>The last leaf is called the terminal leaf</li> </ul> </li> </ul> </li> </ul>"},{"location":"stats-reg/#support-vector-regression-model","title":"Support Vector Regression Model","text":"<ul> <li>Disregards any error within the \\(\\varepsilon\\) insensitive cube (a cube with a buffer area around the regression line)</li> <li> <p>Care about the errors above(\\(\\xi\\)) and below(\\(\\xi^*\\)) the cube area</p> <ul> <li>These error points are vectors and are referred to as support vectors. Also see Support Vectors in Classification</li> </ul> </li> </ul>"},{"location":"stats-reg/#measures-of-variation","title":"Measures of Variation","text":""},{"location":"stats-reg/#sum-of-squared-residuals","title":"Sum of Squared Residuals","text":"<p>To find the best fitting regression line that shows the trend in the data, we want to minimize all the residual values, because doing so would minimize the distance of the data points from the line of best fit. </p> <p>In order to minimize the residual, we actually minimize the square of the residuals so that the positive and negative values of the residuals do not cancel out and all residuals get minimized.</p> <p>This form of regression is also referred to as Ordinary Least Squares Regression as we are trying to minimize the squae of the residuals.</p>"},{"location":"stats-reg/#coefficient-of-determination-r2","title":"Coefficient of Determination, \\(R^2\\)","text":"<p>Measures the percentage of error we eliminated by using least-squares regression instead of just mean,\\(\\overline y\\). </p> <ul> <li> <p>For Simple Linear Regression, square of the Correlation Coefficient (\\(r^2\\)) is used instead of \\(R^2\\). </p> </li> <li> <p>Tells us how well the regression line approximates the data</p> </li> </ul> <p>Tip</p> <ul> <li>Ranges between 0 and 1</li> <li>Expressed as percentage<ul> <li>100% describes a line that is a perfect fit to data<ul> <li>Very rare (almost non -existant) for ML models</li> </ul> </li> <li>The higher the value, the more data points pass through the line</li> <li>Very small values indicate that the regression line does not pass through many data points</li> </ul> </li> </ul>"},{"location":"stats-reg/#adjusted-r-squared","title":"Adjusted R-Squared","text":"<ul> <li>Better measure for model evaluation</li> <li>Sensitive to the number of independent variables in the model</li> <li>Penalizes the model when increasing independent variables<ul> <li>As we put more variables into the model, R-squared increases even if those variables are unrelated to the outcome<ul> <li>OLS tries to predict variables that reduce the residuals</li> <li>Sets coefficients of additional independent variables to 0 if they don't contribute to reducing residuals</li> </ul> </li> <li>Adjusted R-squared attempts to correct for this by deflating R-squared based on increase of additional predictors</li> </ul> </li> </ul>"},{"location":"stats-reg/#root-mean-square-error-rmse","title":"Root Mean Square Error (RMSE)","text":"<p>The standard deviation of the residuals</p> <ul> <li>Also called the Root Mean Square Deviation(RMSD)</li> </ul> <p>Tip</p> <ul> <li>Can be thought of as lines representing the standard deviation of the residuals<ul> <li>Parallel to the regression line</li> <li>Distance between the lines represents the fit<ul> <li>The lesser the distance, the better the fit</li> </ul> </li> </ul> </li> <li>The concept of normal distribution probability based on standard deviations(see here) can be applied here<ul> <li>68.2% of the data points will fall between 1 * RMSE of the regression line</li> <li>95.4% of the data points will fall between 2 * RMSE of the regression line</li> <li>99.7% of the data points will fall between 3 * RMSE of the regression line</li> </ul> </li> </ul>"},{"location":"stats-rv/","title":"Random Variables","text":""},{"location":"stats-rv/#random-variables","title":"Random Variables","text":"<p>Discrete Random Variable: A variable that can only take discrete values</p> <p>Continuous Random Variables: A variable that can table on any value in a certain interval.</p> <p>Expected Value: The mean of a Random Variable.</p>"},{"location":"stats-rv/#binomial-random-variables","title":"Binomial Random Variables","text":"<p>A variable that can take on exactly two values, like a coin flip. </p> <p>Info</p> <p>In order for a variable X to be a binomial random variable,</p> <ul> <li>each trial must be independent,</li> <li>each trial can have only two outcomes - a \u201csuccess\u201d or \u201cfailure,\u201d</li> <li>there are a fixed number of trials, and</li> <li>the probability of success on each trial is constant.</li> </ul> <p>These follow the Binomial Distribution. Also see Binomial Probability</p>"},{"location":"stats-rv/#bernoulli-random-variables","title":"Bernoulli Random Variables","text":"<p>A special category of Binomial RV with exactly one trial with possible outcomes {0, 1} where 0 signifies failure and 1 signifies success.</p> <p>The PMF distrbution for a Bernoulli RV will be the distribution for \\(P(X=0)\\) and \\(P(X=1)\\).</p>"},{"location":"stats-rv/#geometric-random-variables","title":"Geometric Random Variables","text":"<p>Unlike the Binomial RV where we decide the number of trials ahead of time, in case of Geometric RV, we run an infinite number of trials until we get a success. The probablity of success on the nth attempt is given by the Geometric Probability</p> <p>The other three conditions for the Binomial RV are applicable to Geometric RV as well.</p> <p>These follow the Geometric Distribution.</p>"},{"location":"stats-sampling/","title":"Sampling","text":""},{"location":"stats-sampling/#central-limit-theorem-clt","title":"Central Limit Theorem (CLT)","text":"<p>Let \\(X_1, X_2,\u2026, X_n\\) be random samples from a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then even if the population distribution is non-normal (or if the shape of the population distribution is not known), as long as the sample is sufficiently large (\\(n\\geq 30\\)), \\(\\overline X\\) has approximately a normal distribution and the distribution of the probabilities of the means of these samples (SDSM) also follows a normal distribution,</p> <p>The larger the value of \\(n\\), the better the approximation.</p>"},{"location":"stats-sampling/#sampling-concepts","title":"Sampling Concepts","text":"<p>Tip</p> <p>When sampling with replacement, the number of possible samples when using a sample size of \\(n\\) in a population \\(N\\) is \\(N^n\\)</p>"},{"location":"stats-sampling/#finite-correction-factor-fpc","title":"Finite Correction Factor (FPC)","text":"<p>Correction factor applied when sampling</p> <ul> <li>Without replacement</li> <li>Infinite population</li> <li>From more than 5% of a finite population (\\(\\frac{n}{N} \\leq 0.05\\))</li> </ul>"},{"location":"stats-sampling/#sampling-distribution-of-the-sample-meansdsm","title":"Sampling Distribution of the Sample Mean(SDSM)","text":"<p>The probability distribution of all possible sample means for a certain sample size \\(n\\)</p> <p>Conditions for Inference with SDSM</p> <p>Normal Condition</p> <ul> <li>Original population is normally distributed AND/OR</li> <li>\\(n \\geq 30\\)</li> </ul> <p>Independent Condition</p> <ul> <li>Sample with replacement AND/OR</li> <li>\\(n \\leq \\frac{N}{10}\\) (Sample size less than 10% of total population - 10% Rule)</li> </ul>"},{"location":"stats-sampling/#standard-errorse","title":"Standard Error(SE)","text":"<p>Gives an idea of how accurate the mean of a any given sample is likely to be as an estimate of the actual population mean. It is calculated as the Population Standard Deviation divided by the square root of the sample size (see Standard Deviation tab here).</p> <ul> <li>Larger SE indicates that the sample means are more spread out, so it\u2019s less likely that any given sample mean is an accurate representation of the true population mean</li> <li>Smaller SE indicates that the sample means are less spread out, so it\u2019s more likely that any given sample mean is an accurate representation of the true population mean </li> </ul>"},{"location":"stats-sampling/#sampling-distribution-of-the-sample-proportionsdsp","title":"Sampling Distribution of the Sample Proportion(SDSP)","text":"<p>The probability distribution of all possible sample proportions for a certain sample size \\(n\\)</p> <p>Sample Proportion - The proportion of subjects in the sample that meet a cetain condition  where \\(x\\) is the number meeting the condition and \\(n\\) is the sample size</p> <p>Conditions for Inference with SDSP</p> <p>Normal Condition</p> <ul> <li>Original population is normally distributed AND/OR</li> <li>\\(n \\hat p \\geq 10\\) and \\(n(1 - \\hat p) \\geq 10 \\implies np \\geq 10\\) and \\(nq \\geq 10\\)</li> </ul> <p>Independent Condition</p> <ul> <li>Sample with replacement AND/OR</li> <li>\\(n \\leq \\frac{N}{10}\\) (Sample size less than 10% of total population - 10% Rule)</li> </ul>"},{"location":"stats-sampling/#working-with-multiple-samples","title":"Working With Multiple Samples","text":""},{"location":"stats-sampling/#sample-distribution-of-the-difference-of-means-sddm","title":"Sample Distribution of the Difference of Means (SDDM)","text":"<p>The probability distribution of every possible difference of means for certain sample sizes \\(n_1\\) and \\(n_2\\) from populations 1 and 2 respectively</p> <p>Also see: </p> <ul> <li>CI for Difference of Means</li> <li>Test Statistic for Difference of Means</li> </ul>"},{"location":"stats-sampling/#pooled-procedures","title":"Pooled Procedures","text":"<p>If the sample variances are equal or almost equal, then we assume that the population variances are approximately equal as well, and we calculate a pooled variance by combining the two sample variances into one.</p> <ul> <li>Results in smaller \\(\\beta\\) for the same \\(\\alpha\\)</li> <li>As a rule of thumb, we can use pooled variance when the two samples were taken from the same population, or when neither sample variance is more than twice the other.</li> </ul> <p>Also see: </p> <ul> <li>CI for Pooled Procedures</li> <li>Test Statistic for Pooled Procedures</li> </ul>"},{"location":"stats-sampling/#dependent-samples","title":"Dependent Samples","text":"<p>Samples for which the observations from one sample are related to an the observations from the other sample</p> <ul> <li>Hypothesis test with dependent samples is referred to as Matched-pair test</li> </ul> <p>Also see: </p> <ul> <li>CI for Matched-Pair Test</li> <li>Test Statistic for Matched-Pair Test</li> </ul>"},{"location":"stats-sampling/#sample-distribution-of-the-difference-of-proportions-sddp","title":"Sample Distribution of the Difference of Proportions (SDDP)","text":"<p>The probability distribution of every possible difference of proportions for certain sample sizes \\(n_1\\) and \\(n_2\\) from populations 1 and 2 respectively</p> <p>Also see: </p> <ul> <li>CI for Difference of Proportions</li> <li>Test Statistic for Difference of Proportions</li> </ul>"}]}